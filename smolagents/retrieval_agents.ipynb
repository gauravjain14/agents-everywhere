{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        InferenceClient\n",
      "\u001b[0;31mString form:\u001b[0m <InferenceClient(model='Qwen/Qwen2.5-Coder-32B-Instruct', timeout=120)>\n",
      "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/lib/python3.11/site-packages/huggingface_hub/inference/_client.py\n",
      "\u001b[0;31mSource:\u001b[0m     \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mInferenceClient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Initialize a new Inference Client.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    [`InferenceClient`] aims to provide a unified experience to perform inference. The client can be used\u001b[0m\n",
      "\u001b[0;34m    seamlessly with either the (free) Inference API or self-hosted Inference Endpoints.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        model (`str`, `optional`):\u001b[0m\n",
      "\u001b[0;34m            The model to run inference with. Can be a model id hosted on the Hugging Face Hub, e.g. `meta-llama/Meta-Llama-3-8B-Instruct`\u001b[0m\n",
      "\u001b[0;34m            or a URL to a deployed Inference Endpoint. Defaults to None, in which case a recommended model is\u001b[0m\n",
      "\u001b[0;34m            automatically selected for the task.\u001b[0m\n",
      "\u001b[0;34m            Note: for better compatibility with OpenAI's client, `model` has been aliased as `base_url`. Those 2\u001b[0m\n",
      "\u001b[0;34m            arguments are mutually exclusive. If using `base_url` for chat completion, the `/chat/completions` suffix\u001b[0m\n",
      "\u001b[0;34m            path will be appended to the base URL (see the [TGI Messages API](https://huggingface.co/docs/text-generation-inference/en/messages_api)\u001b[0m\n",
      "\u001b[0;34m            documentation for details). When passing a URL as `model`, the client will not append any suffix path to it.\u001b[0m\n",
      "\u001b[0;34m        token (`str` or `bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m            Hugging Face token. Will default to the locally saved token if not provided.\u001b[0m\n",
      "\u001b[0;34m            Pass `token=False` if you don't want to send your token to the server.\u001b[0m\n",
      "\u001b[0;34m            Note: for better compatibility with OpenAI's client, `token` has been aliased as `api_key`. Those 2\u001b[0m\n",
      "\u001b[0;34m            arguments are mutually exclusive and have the exact same behavior.\u001b[0m\n",
      "\u001b[0;34m        timeout (`float`, `optional`):\u001b[0m\n",
      "\u001b[0;34m            The maximum number of seconds to wait for a response from the server. Loading a new model in Inference\u001b[0m\n",
      "\u001b[0;34m            API can take up to several minutes. Defaults to None, meaning it will loop until the server is available.\u001b[0m\n",
      "\u001b[0;34m        headers (`Dict[str, str]`, `optional`):\u001b[0m\n",
      "\u001b[0;34m            Additional headers to send to the server. By default only the authorization and user-agent headers are sent.\u001b[0m\n",
      "\u001b[0;34m            Values in this dictionary will override the default values.\u001b[0m\n",
      "\u001b[0;34m        cookies (`Dict[str, str]`, `optional`):\u001b[0m\n",
      "\u001b[0;34m            Additional cookies to send to the server.\u001b[0m\n",
      "\u001b[0;34m        proxies (`Any`, `optional`):\u001b[0m\n",
      "\u001b[0;34m            Proxies to use for the request.\u001b[0m\n",
      "\u001b[0;34m        base_url (`str`, `optional`):\u001b[0m\n",
      "\u001b[0;34m            Base URL to run inference. This is a duplicated argument from `model` to make [`InferenceClient`]\u001b[0m\n",
      "\u001b[0;34m            follow the same pattern as `openai.OpenAI` client. Cannot be used if `model` is set. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m        api_key (`str`, `optional`):\u001b[0m\n",
      "\u001b[0;34m            Token to use for authentication. This is a duplicated argument from `token` to make [`InferenceClient`]\u001b[0m\n",
      "\u001b[0;34m            follow the same pattern as `openai.OpenAI` client. Cannot be used if `token` is set. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mheaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcookies\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mproxies\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# OpenAI compatibility\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbase_url\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Received both `model` and `base_url` arguments. Please provide only one of them.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" `base_url` is an alias for `model` to make the API compatible with OpenAI's client.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" If using `base_url` for chat completion, the `/chat/completions` suffix path will be appended to the base url.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" When passing a URL as `model`, the client will not append any suffix path to it.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Received both `token` and `api_key` arguments. Please provide only one of them.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" `api_key` is an alias for `token` to make the API compatible with OpenAI's client.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" It has the exact same behavior as `token`.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCaseInsensitiveDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCaseInsensitiveDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbuild_hf_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 'authorization' + 'user-agent'\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcookies\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# OpenAI compatibility\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"<InferenceClient(model='{self.model if self.model else ''}', timeout={self.timeout})>\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mjson\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mContentT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mjson\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mContentT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mjson\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mContentT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mjson\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mContentT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Make a POST request to the inference server.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            json (`Union[str, Dict, List]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The JSON data to send in the request body, specific to each task. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            data (`Union[str, Path, bytes, BinaryIO]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The content to send in the request body, specific to each task.\u001b[0m\n",
      "\u001b[0;34m                It can be raw bytes, a pointer to an opened file, a local file path,\u001b[0m\n",
      "\u001b[0;34m                or a URL to an online resource (image, audio file,...). If both `json` and `data` are passed,\u001b[0m\n",
      "\u001b[0;34m                `data` will take precedence. At least `json` or `data` must be provided. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. Will override the model defined at the instance level. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            task (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The task to perform on the inference. All available tasks can be found\u001b[0m\n",
      "\u001b[0;34m                [here](https://huggingface.co/tasks). Used only to default to a recommended model if `model` is not\u001b[0m\n",
      "\u001b[0;34m                provided. At least `model` or `task` must be provided. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            stream (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to iterate over streaming APIs.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            bytes: The raw bytes returned by the server.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mjson\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ignoring `json` as `data` is passed as binary.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Set Accept header if relevant\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTASKS_EXPECTING_IMAGES\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"Accept\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Accept\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"image/png\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mwith\u001b[0m \u001b[0m_open_as_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_as_binary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_as_binary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;31m# Convert any `TimeoutError` to a `InferenceTimeoutError`\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mraise\u001b[0m \u001b[0mInferenceTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Inference call timed out: {url}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merror\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m422\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;34mf\"{error.args[0]}\\nMake sure '{task}' task is supported by the model.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m503\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;31m# If Model is unavailable, either raise a TimeoutError...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mraise\u001b[0m \u001b[0mInferenceTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0;34mf\"Model not loaded on the server: {url}. Please retry with a higher timeout (current:\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0;34mf\" {self.timeout}).\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;31m# ...or wait 1s and retry\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Waiting for model to be loaded on the server: {error}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0;34m\"X-wait-for-model\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFERENCE_ENDPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X-wait-for-model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0maudio_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0maudio\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AudioClassificationOutputTransform\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAudioClassificationOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Perform audio classification on the provided audio content.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            audio (Union[str, Path, bytes, BinaryIO]):\u001b[0m\n",
      "\u001b[0;34m                The audio content to classify. It can be raw audio bytes, a local audio file, or a URL pointing to an\u001b[0m\n",
      "\u001b[0;34m                audio file.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for audio classification. Can be a model ID hosted on the Hugging Face Hub\u001b[0m\n",
      "\u001b[0;34m                or a URL to a deployed Inference Endpoint. If not provided, the default recommended model for\u001b[0m\n",
      "\u001b[0;34m                audio classification will be used.\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                When specified, limits the output to the top K most probable classes.\u001b[0m\n",
      "\u001b[0;34m            function_to_apply (`\"AudioClassificationOutputTransform\"`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The function to apply to the model outputs in order to retrieve the scores.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[AudioClassificationOutputElement]`: List of [`AudioClassificationOutputElement`] items containing the predicted labels and their confidence.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.audio_classification(\"audio.flac\")\u001b[0m\n",
      "\u001b[0;34m        [\u001b[0m\n",
      "\u001b[0;34m            AudioClassificationOutputElement(score=0.4976358711719513, label='hap'),\u001b[0m\n",
      "\u001b[0;34m            AudioClassificationOutputElement(score=0.3677836060523987, label='neu'),\u001b[0m\n",
      "\u001b[0;34m            ...\u001b[0m\n",
      "\u001b[0;34m        ]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"function_to_apply\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"audio-classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mAudioClassificationOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0maudio_to_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0maudio\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAudioToAudioOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Performs multiple tasks related to audio-to-audio depending on the model (eg: speech enhancement, source separation).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            audio (Union[str, Path, bytes, BinaryIO]):\u001b[0m\n",
      "\u001b[0;34m                The audio content for the model. It can be raw audio bytes, a local audio file, or a URL pointing to an\u001b[0m\n",
      "\u001b[0;34m                audio file.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model can be any model which takes an audio file and returns another audio file. Can be a model ID hosted on the Hugging Face Hub\u001b[0m\n",
      "\u001b[0;34m                or a URL to a deployed Inference Endpoint. If not provided, the default recommended model for\u001b[0m\n",
      "\u001b[0;34m                audio_to_audio will be used.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[AudioToAudioOutputElement]`: A list of [`AudioToAudioOutputElement`] items containing audios label, content-type, and audio content in blob.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            `InferenceTimeoutError`:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> audio_output = client.audio_to_audio(\"audio.flac\")\u001b[0m\n",
      "\u001b[0;34m        >>> for i, item in enumerate(audio_output):\u001b[0m\n",
      "\u001b[0;34m        >>>     with open(f\"output_{i}.flac\", \"wb\") as f:\u001b[0m\n",
      "\u001b[0;34m                    f.write(item.blob)\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"audio-to-audio\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0maudio_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioToAudioOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maudio_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0maudio_output\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mautomatic_speech_recognition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0maudio\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAutomaticSpeechRecognitionOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Perform automatic speech recognition (ASR or audio-to-text) on the given audio content.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            audio (Union[str, Path, bytes, BinaryIO]):\u001b[0m\n",
      "\u001b[0;34m                The content to transcribe. It can be raw audio bytes, local audio file, or a URL to an audio file.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for ASR. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. If not provided, the default recommended model for ASR will be used.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            [`AutomaticSpeechRecognitionOutput`]: An item containing the transcribed text and optionally the timestamp chunks.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.automatic_speech_recognition(\"hello_world.flac\").text\u001b[0m\n",
      "\u001b[0;34m        \"hello world\"\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"automatic-speech-recognition\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mAutomaticSpeechRecognitionOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogit_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputGrammarType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputStreamOptions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtool_choice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputToolChoiceClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ChatCompletionInputToolChoiceEnum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtool_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputTool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_logprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mChatCompletionOutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogit_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputGrammarType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputStreamOptions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtool_choice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputToolChoiceClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ChatCompletionInputToolChoiceEnum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtool_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputTool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_logprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionStreamOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogit_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputGrammarType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputStreamOptions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtool_choice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputToolChoiceClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ChatCompletionInputToolChoiceEnum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtool_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputTool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_logprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionStreamOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Parameters from ChatCompletionInput (handled manually)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogit_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputGrammarType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputStreamOptions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtool_choice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputToolChoiceClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ChatCompletionInputToolChoiceEnum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtool_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionInputTool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_logprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatCompletionStreamOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        A method for completing conversations using a specified language model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        <Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        The `client.chat_completion` method is aliased as `client.chat.completions.create` for compatibility with OpenAI's client.\u001b[0m\n",
      "\u001b[0;34m        Inputs and outputs are strictly the same and using either syntax will yield the same results.\u001b[0m\n",
      "\u001b[0;34m        Check out the [Inference guide](https://huggingface.co/docs/huggingface_hub/guides/inference#openai-compatibility)\u001b[0m\n",
      "\u001b[0;34m        for more details about OpenAI's compatibility.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        </Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            messages (List of [`ChatCompletionInputMessage`]):\u001b[0m\n",
      "\u001b[0;34m                Conversation history consisting of roles and content pairs.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for chat-completion. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. If not provided, the default recommended model for chat-based text-generation will be used.\u001b[0m\n",
      "\u001b[0;34m                See https://huggingface.co/tasks/text-generation for more details.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                If `model` is a model ID, it is passed to the server as the `model` parameter. If you want to define a\u001b[0m\n",
      "\u001b[0;34m                custom URL while setting `model` in the request payload, you must set `base_url` when initializing [`InferenceClient`].\u001b[0m\n",
      "\u001b[0;34m            frequency_penalty (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Penalizes new tokens based on their existing frequency\u001b[0m\n",
      "\u001b[0;34m                in the text so far. Range: [-2.0, 2.0]. Defaults to 0.0.\u001b[0m\n",
      "\u001b[0;34m            logit_bias (`List[float]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens\u001b[0m\n",
      "\u001b[0;34m                (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically,\u001b[0m\n",
      "\u001b[0;34m                the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model,\u001b[0m\n",
      "\u001b[0;34m                but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should\u001b[0m\n",
      "\u001b[0;34m                result in a ban or exclusive selection of the relevant token. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            logprobs (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to return log probabilities of the output tokens or not. If true, returns the log\u001b[0m\n",
      "\u001b[0;34m                probabilities of each output token returned in the content of message.\u001b[0m\n",
      "\u001b[0;34m            max_tokens (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Maximum number of tokens allowed in the response. Defaults to 100.\u001b[0m\n",
      "\u001b[0;34m            n (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                UNUSED.\u001b[0m\n",
      "\u001b[0;34m            presence_penalty (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the\u001b[0m\n",
      "\u001b[0;34m                text so far, increasing the model's likelihood to talk about new topics.\u001b[0m\n",
      "\u001b[0;34m            response_format ([`ChatCompletionInputGrammarType`], *optional*):\u001b[0m\n",
      "\u001b[0;34m                Grammar constraints. Can be either a JSONSchema or a regex.\u001b[0m\n",
      "\u001b[0;34m            seed (Optional[`int`], *optional*):\u001b[0m\n",
      "\u001b[0;34m                Seed for reproducible control flow. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            stop (Optional[`str`], *optional*):\u001b[0m\n",
      "\u001b[0;34m                Up to four strings which trigger the end of the response.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            stream (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Enable realtime streaming of responses. Defaults to False.\u001b[0m\n",
      "\u001b[0;34m            stream_options ([`ChatCompletionInputStreamOptions`], *optional*):\u001b[0m\n",
      "\u001b[0;34m                Options for streaming completions.\u001b[0m\n",
      "\u001b[0;34m            temperature (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Controls randomness of the generations. Lower values ensure\u001b[0m\n",
      "\u001b[0;34m                less random completions. Range: [0, 2]. Defaults to 1.0.\u001b[0m\n",
      "\u001b[0;34m            top_logprobs (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                An integer between 0 and 5 specifying the number of most likely tokens to return at each token\u001b[0m\n",
      "\u001b[0;34m                position, each with an associated log probability. logprobs must be set to true if this parameter is\u001b[0m\n",
      "\u001b[0;34m                used.\u001b[0m\n",
      "\u001b[0;34m            top_p (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Fraction of the most likely next words to sample from.\u001b[0m\n",
      "\u001b[0;34m                Must be between 0 and 1. Defaults to 1.0.\u001b[0m\n",
      "\u001b[0;34m            tool_choice ([`ChatCompletionInputToolChoiceClass`] or [`ChatCompletionInputToolChoiceEnum`], *optional*):\u001b[0m\n",
      "\u001b[0;34m                The tool to use for the completion. Defaults to \"auto\".\u001b[0m\n",
      "\u001b[0;34m            tool_prompt (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                A prompt to be appended before the tools.\u001b[0m\n",
      "\u001b[0;34m            tools (List of [`ChatCompletionInputTool`], *optional*):\u001b[0m\n",
      "\u001b[0;34m                A list of tools the model may call. Currently, only functions are supported as a tool. Use this to\u001b[0m\n",
      "\u001b[0;34m                provide a list of functions the model may generate JSON inputs for.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            [`ChatCompletionOutput`] or Iterable of [`ChatCompletionStreamOutput`]:\u001b[0m\n",
      "\u001b[0;34m            Generated text returned from the server:\u001b[0m\n",
      "\u001b[0;34m            - if `stream=False`, the generated text is returned as a [`ChatCompletionOutput`] (default).\u001b[0m\n",
      "\u001b[0;34m            - if `stream=True`, the generated text is returned token by token as a sequence of [`ChatCompletionStreamOutput`].\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient(\"meta-llama/Meta-Llama-3-8B-Instruct\")\u001b[0m\n",
      "\u001b[0;34m        >>> client.chat_completion(messages, max_tokens=100)\u001b[0m\n",
      "\u001b[0;34m        ChatCompletionOutput(\u001b[0m\n",
      "\u001b[0;34m            choices=[\u001b[0m\n",
      "\u001b[0;34m                ChatCompletionOutputComplete(\u001b[0m\n",
      "\u001b[0;34m                    finish_reason='eos_token',\u001b[0m\n",
      "\u001b[0;34m                    index=0,\u001b[0m\n",
      "\u001b[0;34m                    message=ChatCompletionOutputMessage(\u001b[0m\n",
      "\u001b[0;34m                        role='assistant',\u001b[0m\n",
      "\u001b[0;34m                        content='The capital of France is Paris.',\u001b[0m\n",
      "\u001b[0;34m                        name=None,\u001b[0m\n",
      "\u001b[0;34m                        tool_calls=None\u001b[0m\n",
      "\u001b[0;34m                    ),\u001b[0m\n",
      "\u001b[0;34m                    logprobs=None\u001b[0m\n",
      "\u001b[0;34m                )\u001b[0m\n",
      "\u001b[0;34m            ],\u001b[0m\n",
      "\u001b[0;34m            created=1719907176,\u001b[0m\n",
      "\u001b[0;34m            id='',\u001b[0m\n",
      "\u001b[0;34m            model='meta-llama/Meta-Llama-3-8B-Instruct',\u001b[0m\n",
      "\u001b[0;34m            object='text_completion',\u001b[0m\n",
      "\u001b[0;34m            system_fingerprint='2.0.4-sha-f426a33',\u001b[0m\n",
      "\u001b[0;34m            usage=ChatCompletionOutputUsage(\u001b[0m\n",
      "\u001b[0;34m                completion_tokens=8,\u001b[0m\n",
      "\u001b[0;34m                prompt_tokens=17,\u001b[0m\n",
      "\u001b[0;34m                total_tokens=25\u001b[0m\n",
      "\u001b[0;34m            )\u001b[0m\n",
      "\u001b[0;34m        )\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example using streaming:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient(\"meta-llama/Meta-Llama-3-8B-Instruct\")\u001b[0m\n",
      "\u001b[0;34m        >>> for token in client.chat_completion(messages, max_tokens=10, stream=True):\u001b[0m\n",
      "\u001b[0;34m        ...     print(token)\u001b[0m\n",
      "\u001b[0;34m        ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(content='The', role='assistant'), index=0, finish_reason=None)], created=1710498504)\u001b[0m\n",
      "\u001b[0;34m        ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(content=' capital', role='assistant'), index=0, finish_reason=None)], created=1710498504)\u001b[0m\n",
      "\u001b[0;34m        (...)\u001b[0m\n",
      "\u001b[0;34m        ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(content=' may', role='assistant'), index=0, finish_reason=None)], created=1710498504)\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example using OpenAI's syntax:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        # instead of `from openai import OpenAI`\u001b[0m\n",
      "\u001b[0;34m        from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        # instead of `client = OpenAI(...)`\u001b[0m\n",
      "\u001b[0;34m        client = InferenceClient(\u001b[0m\n",
      "\u001b[0;34m            base_url=...,\u001b[0m\n",
      "\u001b[0;34m            api_key=...,\u001b[0m\n",
      "\u001b[0;34m        )\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        output = client.chat.completions.create(\u001b[0m\n",
      "\u001b[0;34m            model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\u001b[0m\n",
      "\u001b[0;34m            messages=[\u001b[0m\n",
      "\u001b[0;34m                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\u001b[0m\n",
      "\u001b[0;34m                {\"role\": \"user\", \"content\": \"Count to 10\"},\u001b[0m\n",
      "\u001b[0;34m            ],\u001b[0m\n",
      "\u001b[0;34m            stream=True,\u001b[0m\n",
      "\u001b[0;34m            max_tokens=1024,\u001b[0m\n",
      "\u001b[0;34m        )\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        for chunk in output:\u001b[0m\n",
      "\u001b[0;34m            print(chunk.choices[0].delta.content)\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example using Image + Text as input:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        # provide a remote URL\u001b[0m\n",
      "\u001b[0;34m        >>> image_url =\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\u001b[0m\n",
      "\u001b[0;34m        # or a base64-encoded image\u001b[0m\n",
      "\u001b[0;34m        >>> image_path = \"/path/to/image.jpeg\"\u001b[0m\n",
      "\u001b[0;34m        >>> with open(image_path, \"rb\") as f:\u001b[0m\n",
      "\u001b[0;34m        ...     base64_image = base64.b64encode(f.read()).decode(\"utf-8\")\u001b[0m\n",
      "\u001b[0;34m        >>> image_url = f\"data:image/jpeg;base64,{base64_image}\"\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient(\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\u001b[0m\n",
      "\u001b[0;34m        >>> output = client.chat.completions.create(\u001b[0m\n",
      "\u001b[0;34m        ...     messages=[\u001b[0m\n",
      "\u001b[0;34m        ...         {\u001b[0m\n",
      "\u001b[0;34m        ...             \"role\": \"user\",\u001b[0m\n",
      "\u001b[0;34m        ...             \"content\": [\u001b[0m\n",
      "\u001b[0;34m        ...                 {\u001b[0m\n",
      "\u001b[0;34m        ...                     \"type\": \"image_url\",\u001b[0m\n",
      "\u001b[0;34m        ...                     \"image_url\": {\"url\": image_url},\u001b[0m\n",
      "\u001b[0;34m        ...                 },\u001b[0m\n",
      "\u001b[0;34m        ...                 {\u001b[0m\n",
      "\u001b[0;34m        ...                     \"type\": \"text\",\u001b[0m\n",
      "\u001b[0;34m        ...                     \"text\": \"Describe this image in one sentence.\",\u001b[0m\n",
      "\u001b[0;34m        ...                 },\u001b[0m\n",
      "\u001b[0;34m        ...             ],\u001b[0m\n",
      "\u001b[0;34m        ...         },\u001b[0m\n",
      "\u001b[0;34m        ...     ],\u001b[0m\n",
      "\u001b[0;34m        ... )\u001b[0m\n",
      "\u001b[0;34m        >>> output\u001b[0m\n",
      "\u001b[0;34m        The image depicts the iconic Statue of Liberty situated in New York Harbor, New York, on a clear day.\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example using tools:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient(\"meta-llama/Meta-Llama-3-70B-Instruct\")\u001b[0m\n",
      "\u001b[0;34m        >>> messages = [\u001b[0m\n",
      "\u001b[0;34m        ...     {\u001b[0m\n",
      "\u001b[0;34m        ...         \"role\": \"system\",\u001b[0m\n",
      "\u001b[0;34m        ...         \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\",\u001b[0m\n",
      "\u001b[0;34m        ...     },\u001b[0m\n",
      "\u001b[0;34m        ...     {\u001b[0m\n",
      "\u001b[0;34m        ...         \"role\": \"user\",\u001b[0m\n",
      "\u001b[0;34m        ...         \"content\": \"What's the weather like the next 3 days in San Francisco, CA?\",\u001b[0m\n",
      "\u001b[0;34m        ...     },\u001b[0m\n",
      "\u001b[0;34m        ... ]\u001b[0m\n",
      "\u001b[0;34m        >>> tools = [\u001b[0m\n",
      "\u001b[0;34m        ...     {\u001b[0m\n",
      "\u001b[0;34m        ...         \"type\": \"function\",\u001b[0m\n",
      "\u001b[0;34m        ...         \"function\": {\u001b[0m\n",
      "\u001b[0;34m        ...             \"name\": \"get_current_weather\",\u001b[0m\n",
      "\u001b[0;34m        ...             \"description\": \"Get the current weather\",\u001b[0m\n",
      "\u001b[0;34m        ...             \"parameters\": {\u001b[0m\n",
      "\u001b[0;34m        ...                 \"type\": \"object\",\u001b[0m\n",
      "\u001b[0;34m        ...                 \"properties\": {\u001b[0m\n",
      "\u001b[0;34m        ...                     \"location\": {\u001b[0m\n",
      "\u001b[0;34m        ...                         \"type\": \"string\",\u001b[0m\n",
      "\u001b[0;34m        ...                         \"description\": \"The city and state, e.g. San Francisco, CA\",\u001b[0m\n",
      "\u001b[0;34m        ...                     },\u001b[0m\n",
      "\u001b[0;34m        ...                     \"format\": {\u001b[0m\n",
      "\u001b[0;34m        ...                         \"type\": \"string\",\u001b[0m\n",
      "\u001b[0;34m        ...                         \"enum\": [\"celsius\", \"fahrenheit\"],\u001b[0m\n",
      "\u001b[0;34m        ...                         \"description\": \"The temperature unit to use. Infer this from the users location.\",\u001b[0m\n",
      "\u001b[0;34m        ...                     },\u001b[0m\n",
      "\u001b[0;34m        ...                 },\u001b[0m\n",
      "\u001b[0;34m        ...                 \"required\": [\"location\", \"format\"],\u001b[0m\n",
      "\u001b[0;34m        ...             },\u001b[0m\n",
      "\u001b[0;34m        ...         },\u001b[0m\n",
      "\u001b[0;34m        ...     },\u001b[0m\n",
      "\u001b[0;34m        ...     {\u001b[0m\n",
      "\u001b[0;34m        ...         \"type\": \"function\",\u001b[0m\n",
      "\u001b[0;34m        ...         \"function\": {\u001b[0m\n",
      "\u001b[0;34m        ...             \"name\": \"get_n_day_weather_forecast\",\u001b[0m\n",
      "\u001b[0;34m        ...             \"description\": \"Get an N-day weather forecast\",\u001b[0m\n",
      "\u001b[0;34m        ...             \"parameters\": {\u001b[0m\n",
      "\u001b[0;34m        ...                 \"type\": \"object\",\u001b[0m\n",
      "\u001b[0;34m        ...                 \"properties\": {\u001b[0m\n",
      "\u001b[0;34m        ...                     \"location\": {\u001b[0m\n",
      "\u001b[0;34m        ...                         \"type\": \"string\",\u001b[0m\n",
      "\u001b[0;34m        ...                         \"description\": \"The city and state, e.g. San Francisco, CA\",\u001b[0m\n",
      "\u001b[0;34m        ...                     },\u001b[0m\n",
      "\u001b[0;34m        ...                     \"format\": {\u001b[0m\n",
      "\u001b[0;34m        ...                         \"type\": \"string\",\u001b[0m\n",
      "\u001b[0;34m        ...                         \"enum\": [\"celsius\", \"fahrenheit\"],\u001b[0m\n",
      "\u001b[0;34m        ...                         \"description\": \"The temperature unit to use. Infer this from the users location.\",\u001b[0m\n",
      "\u001b[0;34m        ...                     },\u001b[0m\n",
      "\u001b[0;34m        ...                     \"num_days\": {\u001b[0m\n",
      "\u001b[0;34m        ...                         \"type\": \"integer\",\u001b[0m\n",
      "\u001b[0;34m        ...                         \"description\": \"The number of days to forecast\",\u001b[0m\n",
      "\u001b[0;34m        ...                     },\u001b[0m\n",
      "\u001b[0;34m        ...                 },\u001b[0m\n",
      "\u001b[0;34m        ...                 \"required\": [\"location\", \"format\", \"num_days\"],\u001b[0m\n",
      "\u001b[0;34m        ...             },\u001b[0m\n",
      "\u001b[0;34m        ...         },\u001b[0m\n",
      "\u001b[0;34m        ...     },\u001b[0m\n",
      "\u001b[0;34m        ... ]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> response = client.chat_completion(\u001b[0m\n",
      "\u001b[0;34m        ...     model=\"meta-llama/Meta-Llama-3-70B-Instruct\",\u001b[0m\n",
      "\u001b[0;34m        ...     messages=messages,\u001b[0m\n",
      "\u001b[0;34m        ...     tools=tools,\u001b[0m\n",
      "\u001b[0;34m        ...     tool_choice=\"auto\",\u001b[0m\n",
      "\u001b[0;34m        ...     max_tokens=500,\u001b[0m\n",
      "\u001b[0;34m        ... )\u001b[0m\n",
      "\u001b[0;34m        >>> response.choices[0].message.tool_calls[0].function\u001b[0m\n",
      "\u001b[0;34m        ChatCompletionOutputFunctionDefinition(\u001b[0m\n",
      "\u001b[0;34m            arguments={\u001b[0m\n",
      "\u001b[0;34m                'location': 'San Francisco, CA',\u001b[0m\n",
      "\u001b[0;34m                'format': 'fahrenheit',\u001b[0m\n",
      "\u001b[0;34m                'num_days': 3\u001b[0m\n",
      "\u001b[0;34m            },\u001b[0m\n",
      "\u001b[0;34m            name='get_n_day_weather_forecast',\u001b[0m\n",
      "\u001b[0;34m            description=None\u001b[0m\n",
      "\u001b[0;34m        )\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example using response_format:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient(\"meta-llama/Meta-Llama-3-70B-Instruct\")\u001b[0m\n",
      "\u001b[0;34m        >>> messages = [\u001b[0m\n",
      "\u001b[0;34m        ...     {\u001b[0m\n",
      "\u001b[0;34m        ...         \"role\": \"user\",\u001b[0m\n",
      "\u001b[0;34m        ...         \"content\": \"I saw a puppy a cat and a raccoon during my bike ride in the park. What did I saw and when?\",\u001b[0m\n",
      "\u001b[0;34m        ...     },\u001b[0m\n",
      "\u001b[0;34m        ... ]\u001b[0m\n",
      "\u001b[0;34m        >>> response_format = {\u001b[0m\n",
      "\u001b[0;34m        ...     \"type\": \"json\",\u001b[0m\n",
      "\u001b[0;34m        ...     \"value\": {\u001b[0m\n",
      "\u001b[0;34m        ...         \"properties\": {\u001b[0m\n",
      "\u001b[0;34m        ...             \"location\": {\"type\": \"string\"},\u001b[0m\n",
      "\u001b[0;34m        ...             \"activity\": {\"type\": \"string\"},\u001b[0m\n",
      "\u001b[0;34m        ...             \"animals_seen\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 5},\u001b[0m\n",
      "\u001b[0;34m        ...             \"animals\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\u001b[0m\n",
      "\u001b[0;34m        ...         },\u001b[0m\n",
      "\u001b[0;34m        ...         \"required\": [\"location\", \"activity\", \"animals_seen\", \"animals\"],\u001b[0m\n",
      "\u001b[0;34m        ...     },\u001b[0m\n",
      "\u001b[0;34m        ... }\u001b[0m\n",
      "\u001b[0;34m        >>> response = client.chat_completion(\u001b[0m\n",
      "\u001b[0;34m        ...     messages=messages,\u001b[0m\n",
      "\u001b[0;34m        ...     response_format=response_format,\u001b[0m\n",
      "\u001b[0;34m        ...     max_tokens=500,\u001b[0m\n",
      "\u001b[0;34m        )\u001b[0m\n",
      "\u001b[0;34m        >>> response.choices[0].message.content\u001b[0m\n",
      "\u001b[0;34m        '{\\n\\n\"activity\": \"bike ride\",\\n\"animals\": [\"puppy\", \"cat\", \"raccoon\"],\\n\"animals_seen\": 3,\\n\"location\": \"park\"}'\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_chat_completion_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# `model` is sent in the payload. Not used by the server but can be useful for debugging/routing.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# If it's a ID on the Hub => use it. Otherwise, we use a random string.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"tgi\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tgi\"\u001b[0m  \u001b[0;31m# dummy value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogit_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogit_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpresence_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mresponse_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtool_choice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_choice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtool_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtop_logprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_logprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstream_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0m_stream_chat_completion_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mChatCompletionOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_resolve_chat_completion_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Since `chat_completion(..., model=xxx)` is also a payload parameter for the server, we need to handle 'model' differently.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# `self.base_url` and `self.model` takes precedence over 'model' argument only in `chat_completion`.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel_id_or_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recommended_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Resolve URL if it's a model ID\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel_id_or_url\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mmodel_id_or_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Strip trailing /\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Append /chat/completions if not already present\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel_url\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Append /v1/chat/completions if not already present\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel_url\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"/v1/chat/completions\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_url\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mdocument_question_answering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdoc_stride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mhandle_impossible_answer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlang\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_answer_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_question_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mword_boxes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocumentQuestionAnsweringOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Answer questions on document images.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            image (`Union[str, Path, bytes, BinaryIO]`):\u001b[0m\n",
      "\u001b[0;34m                The input image for the context. It can be raw bytes, an image file, or a URL to an online image.\u001b[0m\n",
      "\u001b[0;34m            question (`str`):\u001b[0m\n",
      "\u001b[0;34m                Question to be answered.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the document question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint. If not provided, the default recommended document question answering model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            doc_stride (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                If the words in the document are too long to fit with the question for the model, it will be split in\u001b[0m\n",
      "\u001b[0;34m                several chunks with some overlap. This argument controls the size of that overlap.\u001b[0m\n",
      "\u001b[0;34m            handle_impossible_answer (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to accept impossible as an answer\u001b[0m\n",
      "\u001b[0;34m            lang (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Language to use while running OCR. Defaults to english.\u001b[0m\n",
      "\u001b[0;34m            max_answer_len (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\u001b[0m\n",
      "\u001b[0;34m            max_question_len (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The maximum length of the question after tokenization. It will be truncated if needed.\u001b[0m\n",
      "\u001b[0;34m            max_seq_len (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The maximum length of the total sentence (context + question) in tokens of each chunk passed to the\u001b[0m\n",
      "\u001b[0;34m                model. The context will be split in several chunks (using doc_stride as overlap) if needed.\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The number of answers to return (will be chosen by order of likelihood). Can return less than top_k\u001b[0m\n",
      "\u001b[0;34m                answers if there are not enough options available within the context.\u001b[0m\n",
      "\u001b[0;34m            word_boxes (`List[Union[List[float], str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                A list of words and bounding boxes (normalized 0->1000). If provided, the inference will skip the OCR\u001b[0m\n",
      "\u001b[0;34m                step and use the provided bounding boxes instead.\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[DocumentQuestionAnsweringOutputElement]`: a list of [`DocumentQuestionAnsweringOutputElement`] items containing the predicted label, associated probability, word ids, and page number.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.document_question_answering(image=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\", question=\"What is the invoice number?\")\u001b[0m\n",
      "\u001b[0;34m        [DocumentQuestionAnsweringOutputElement(answer='us-001', end=16, score=0.9999666213989258, start=16)]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_b64_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"doc_stride\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc_stride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"handle_impossible_answer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhandle_impossible_answer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"lang\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"max_answer_len\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_answer_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"max_question_len\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_question_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"max_seq_len\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"word_boxes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_boxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"document-question-answering\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mDocumentQuestionAnsweringOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncation_direction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Right\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"np.ndarray\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Generate embeddings for a given text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            text (`str`):\u001b[0m\n",
      "\u001b[0;34m                The text to embed.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the conversational task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint. If not provided, the default recommended conversational model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            normalize (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to normalize the embeddings or not.\u001b[0m\n",
      "\u001b[0;34m                Only available on server powered by Text-Embedding-Inference.\u001b[0m\n",
      "\u001b[0;34m            prompt_name (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The name of the prompt that should be used by for encoding. If not set, no prompt will be applied.\u001b[0m\n",
      "\u001b[0;34m                Must be a key in the `Sentence Transformers` configuration `prompts` dictionary.\u001b[0m\n",
      "\u001b[0;34m                For example if ``prompt_name`` is \"query\" and the ``prompts`` is {\"query\": \"query: \",...},\u001b[0m\n",
      "\u001b[0;34m                then the sentence \"What is the capital of France?\" will be encoded as \"query: What is the capital of France?\"\u001b[0m\n",
      "\u001b[0;34m                because the prompt text will be prepended before any text to encode.\u001b[0m\n",
      "\u001b[0;34m            truncate (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to truncate the embeddings or not.\u001b[0m\n",
      "\u001b[0;34m                Only available on server powered by Text-Embedding-Inference.\u001b[0m\n",
      "\u001b[0;34m            truncation_direction (`Literal[\"Left\", \"Right\"]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Which side of the input should be truncated when `truncate=True` is passed.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `np.ndarray`: The embedding representing the input text as a float32 numpy array.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.feature_extraction(\"Hi, who are you?\")\u001b[0m\n",
      "\u001b[0;34m        array([[ 2.424802  ,  2.93384   ,  1.1750331 , ...,  1.240499, -0.13776633, -0.7889173 ],\u001b[0m\n",
      "\u001b[0;34m        [-0.42943227, -0.6364878 , -1.693462  , ...,  0.41978157, -2.4336355 ,  0.6162071 ],\u001b[0m\n",
      "\u001b[0;34m        ...,\u001b[0m\n",
      "\u001b[0;34m        [ 0.28552425, -0.928395  , -1.2077185 , ...,  0.76810825, -2.1069427 ,  0.6236161 ]], dtype=float32)\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"normalize\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"prompt_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"truncate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"truncation_direction\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtruncation_direction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"feature-extraction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_import_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_bytes_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfill_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFillMaskOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Fill in a hole with a missing word (token to be precise).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            text (`str`):\u001b[0m\n",
      "\u001b[0;34m                a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask).\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the fill mask task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint. If not provided, the default recommended fill mask model will be used.\u001b[0m\n",
      "\u001b[0;34m            targets (`List[str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                When passed, the model will limit the scores to the passed targets instead of looking up in the whole\u001b[0m\n",
      "\u001b[0;34m                vocabulary. If the provided targets are not in the model vocab, they will be tokenized and the first\u001b[0m\n",
      "\u001b[0;34m                resulting token will be used (with a warning, and that might be slower).\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                When passed, overrides the number of predictions to return.\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[FillMaskOutputElement]`: a list of [`FillMaskOutputElement`] items containing the predicted label, associated\u001b[0m\n",
      "\u001b[0;34m            probability, token reference, and completed text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.fill_mask(\"The goal of life is <mask>.\")\u001b[0m\n",
      "\u001b[0;34m        [\u001b[0m\n",
      "\u001b[0;34m            FillMaskOutputElement(score=0.06897063553333282, token=11098, token_str=' happiness', sequence='The goal of life is happiness.'),\u001b[0m\n",
      "\u001b[0;34m            FillMaskOutputElement(score=0.06554922461509705, token=45075, token_str=' immortality', sequence='The goal of life is immortality.')\u001b[0m\n",
      "\u001b[0;34m        ]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fill-mask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mFillMaskOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mimage_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ImageClassificationOutputTransform\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mImageClassificationOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Perform image classification on the given image using the specified model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            image (`Union[str, Path, bytes, BinaryIO]`):\u001b[0m\n",
      "\u001b[0;34m                The image to classify. It can be raw bytes, an image file, or a URL to an online image.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for image classification. Can be a model ID hosted on the Hugging Face Hub or a URL to a\u001b[0m\n",
      "\u001b[0;34m                deployed Inference Endpoint. If not provided, the default recommended model for image classification will be used.\u001b[0m\n",
      "\u001b[0;34m            function_to_apply (`\"ImageClassificationOutputTransform\"`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The function to apply to the model outputs in order to retrieve the scores.\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                When specified, limits the output to the top K most probable classes.\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[ImageClassificationOutputElement]`: a list of [`ImageClassificationOutputElement`] items containing the predicted label and associated probability.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.image_classification(\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\")\u001b[0m\n",
      "\u001b[0;34m        [ImageClassificationOutputElement(label='Blenheim spaniel', score=0.9779096841812134), ...]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"function_to_apply\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image-classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mImageClassificationOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mimage_segmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmask_threshold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moverlap_mask_area_threshold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msubtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ImageSegmentationSubtask\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mImageSegmentationOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Perform image segmentation on the given image using the specified model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        <Tip warning={true}>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        You must have `PIL` installed if you want to work with images (`pip install Pillow`).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        </Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            image (`Union[str, Path, bytes, BinaryIO]`):\u001b[0m\n",
      "\u001b[0;34m                The image to segment. It can be raw bytes, an image file, or a URL to an online image.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for image segmentation. Can be a model ID hosted on the Hugging Face Hub or a URL to a\u001b[0m\n",
      "\u001b[0;34m                deployed Inference Endpoint. If not provided, the default recommended model for image segmentation will be used.\u001b[0m\n",
      "\u001b[0;34m            mask_threshold (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Threshold to use when turning the predicted masks into binary values.\u001b[0m\n",
      "\u001b[0;34m            overlap_mask_area_threshold (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Mask overlap threshold to eliminate small, disconnected segments.\u001b[0m\n",
      "\u001b[0;34m            subtask (`\"ImageSegmentationSubtask\"`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Segmentation task to be performed, depending on model capabilities.\u001b[0m\n",
      "\u001b[0;34m            threshold (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Probability threshold to filter out predicted masks.\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[ImageSegmentationOutputElement]`: A list of [`ImageSegmentationOutputElement`] items containing the segmented masks and associated attributes.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.image_segmentation(\"cat.jpg\")\u001b[0m\n",
      "\u001b[0;34m        [ImageSegmentationOutputElement(score=0.989008, label='LABEL_184', mask=<PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>), ...]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"mask_threshold\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmask_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"overlap_mask_area_threshold\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moverlap_mask_area_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"subtask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msubtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"threshold\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image-segmentation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageSegmentationOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_b64_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore [assignment]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mimage_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mguidance_scale\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtarget_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mImageToImageTargetSize\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Perform image-to-image translation using a specified model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        <Tip warning={true}>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        You must have `PIL` installed if you want to work with images (`pip install Pillow`).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        </Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            image (`Union[str, Path, bytes, BinaryIO]`):\u001b[0m\n",
      "\u001b[0;34m                The input image for translation. It can be raw bytes, an image file, or a URL to an online image.\u001b[0m\n",
      "\u001b[0;34m            prompt (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The text prompt to guide the image generation.\u001b[0m\n",
      "\u001b[0;34m            negative_prompt (`List[str]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                One or several prompt to guide what NOT to include in image generation.\u001b[0m\n",
      "\u001b[0;34m            num_inference_steps (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                For diffusion models. The number of denoising steps. More denoising steps usually lead to a higher\u001b[0m\n",
      "\u001b[0;34m                quality image at the expense of slower inference.\u001b[0m\n",
      "\u001b[0;34m            guidance_scale (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                For diffusion models. A higher guidance scale value encourages the model to generate images closely\u001b[0m\n",
      "\u001b[0;34m                linked to the text prompt at the expense of lower image quality.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            target_size (`ImageToImageTargetSize`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The size in pixel of the output image.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `Image`: The translated image.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> image = client.image_to_image(\"cat.jpg\", prompt=\"turn the cat into a tiger\")\u001b[0m\n",
      "\u001b[0;34m        >>> image.save(\"tiger.jpg\")\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"negative_prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"target_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"num_inference_steps\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"guidance_scale\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mguidance_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image-to-image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0m_bytes_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mimage_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mImageToTextOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Takes an input image and return text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Models can have very different outputs depending on your use case (image captioning, optical character recognition\u001b[0m\n",
      "\u001b[0;34m        (OCR), Pix2Struct, etc). Please have a look to the model card to learn more about a model's specificities.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            image (`Union[str, Path, bytes, BinaryIO]`):\u001b[0m\n",
      "\u001b[0;34m                The input image to caption. It can be raw bytes, an image file, or a URL to an online image..\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            [`ImageToTextOutput`]: The generated text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.image_to_text(\"cat.jpg\")\u001b[0m\n",
      "\u001b[0;34m        'a cat standing in a grassy field '\u001b[0m\n",
      "\u001b[0;34m        >>> client.image_to_text(\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\")\u001b[0m\n",
      "\u001b[0;34m        'a dog laying on the grass next to a flower pot '\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image-to-text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageToTextOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mlist_deployed_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframeworks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        List models deployed on the Serverless Inference API service.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        This helper checks deployed models framework by framework. By default, it will check the 4 main frameworks that\u001b[0m\n",
      "\u001b[0;34m        are supported and account for 95% of the hosted models. However, if you want a complete list of models you can\u001b[0m\n",
      "\u001b[0;34m        specify `frameworks=\"all\"` as input. Alternatively, if you know before-hand which framework you are interested\u001b[0m\n",
      "\u001b[0;34m        in, you can also restrict to search to this one (e.g. `frameworks=\"text-generation-inference\"`). The more\u001b[0m\n",
      "\u001b[0;34m        frameworks are checked, the more time it will take.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        <Tip warning={true}>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        This endpoint method does not return a live list of all models available for the Serverless Inference API service.\u001b[0m\n",
      "\u001b[0;34m        It searches over a cached list of models that were recently available and the list may not be up to date.\u001b[0m\n",
      "\u001b[0;34m        If you want to know the live status of a specific model, use [`~InferenceClient.get_model_status`].\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        </Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        <Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        This endpoint method is mostly useful for discoverability. If you already know which model you want to use and want to\u001b[0m\n",
      "\u001b[0;34m        check its availability, you can directly use [`~InferenceClient.get_model_status`].\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        </Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            frameworks (`Literal[\"all\"]` or `List[str]` or `str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The frameworks to filter on. By default only a subset of the available frameworks are tested. If set to\u001b[0m\n",
      "\u001b[0;34m                \"all\", all available frameworks will be tested. It is also possible to provide a single framework or a\u001b[0m\n",
      "\u001b[0;34m                custom set of frameworks to check.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `Dict[str, List[str]]`: A dictionary mapping task names to a sorted list of model IDs.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```python\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        # Discover zero-shot-classification models currently deployed\u001b[0m\n",
      "\u001b[0;34m        >>> models = client.list_deployed_models()\u001b[0m\n",
      "\u001b[0;34m        >>> models[\"zero-shot-classification\"]\u001b[0m\n",
      "\u001b[0;34m        ['Narsil/deberta-large-mnli-zero-cls', 'facebook/bart-large-mnli', ...]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        # List from only 1 framework\u001b[0m\n",
      "\u001b[0;34m        >>> client.list_deployed_models(\"text-generation-inference\")\u001b[0m\n",
      "\u001b[0;34m        {'text-generation': ['bigcode/starcoder', 'meta-llama/Llama-2-70b-chat-hf', ...], ...}\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Resolve which frameworks to check\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mframeworks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mframeworks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAIN_INFERENCE_API_FRAMEWORKS\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mframeworks\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mframeworks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_INFERENCE_API_FRAMEWORKS\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeworks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mframeworks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mframeworks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mframeworks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeworks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Fetch them iteratively\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodels_by_task\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mdef\u001b[0m \u001b[0m_unpack_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sentence-transformers\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;31m# Model running with the `sentence-transformers` framework can work with both tasks even if not\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;31m# branded as such in the API response\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmodels_by_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feature-extraction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmodels_by_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence-similarity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmodels_by_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframeworks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{INFERENCE_ENDPOINT}/framework/{framework}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0m_unpack_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Sort alphabetically for discoverability and return\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_by_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodels_by_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmodels_by_task\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObjectDetectionOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Perform object detection on the given image using the specified model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        <Tip warning={true}>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        You must have `PIL` installed if you want to work with images (`pip install Pillow`).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        </Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            image (`Union[str, Path, bytes, BinaryIO]`):\u001b[0m\n",
      "\u001b[0;34m                The image to detect objects on. It can be raw bytes, an image file, or a URL to an online image.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for object detection. Can be a model ID hosted on the Hugging Face Hub or a URL to a\u001b[0m\n",
      "\u001b[0;34m                deployed Inference Endpoint. If not provided, the default recommended model for object detection (DETR) will be used.\u001b[0m\n",
      "\u001b[0;34m            threshold (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The probability necessary to make a prediction.\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[ObjectDetectionOutputElement]`: A list of [`ObjectDetectionOutputElement`] items containing the bounding boxes and associated attributes.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m            `ValueError`:\u001b[0m\n",
      "\u001b[0;34m                If the request output is not a List.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.object_detection(\"people.jpg\")\u001b[0m\n",
      "\u001b[0;34m        [ObjectDetectionOutputElement(score=0.9486683011054993, label='person', box=ObjectDetectionBoundingBox(xmin=59, ymin=39, xmax=420, ymax=510)), ...]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"threshold\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"object-detection\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mObjectDetectionOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mquestion_answering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0malign_to_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdoc_stride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mhandle_impossible_answer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_answer_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_question_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mQuestionAnsweringOutputElement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mQuestionAnsweringOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Retrieve the answer to a question from a given text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            question (`str`):\u001b[0m\n",
      "\u001b[0;34m                Question to be answered.\u001b[0m\n",
      "\u001b[0;34m            context (`str`):\u001b[0m\n",
      "\u001b[0;34m                The context of the question.\u001b[0m\n",
      "\u001b[0;34m            model (`str`):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint.\u001b[0m\n",
      "\u001b[0;34m            align_to_words (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Attempts to align the answer to real words. Improves quality on space separated languages. Might hurt\u001b[0m\n",
      "\u001b[0;34m                on non-space-separated languages (like Japanese or Chinese)\u001b[0m\n",
      "\u001b[0;34m            doc_stride (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                If the context is too long to fit with the question for the model, it will be split in several chunks\u001b[0m\n",
      "\u001b[0;34m                with some overlap. This argument controls the size of that overlap.\u001b[0m\n",
      "\u001b[0;34m            handle_impossible_answer (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to accept impossible as an answer.\u001b[0m\n",
      "\u001b[0;34m            max_answer_len (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\u001b[0m\n",
      "\u001b[0;34m            max_question_len (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The maximum length of the question after tokenization. It will be truncated if needed.\u001b[0m\n",
      "\u001b[0;34m            max_seq_len (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The maximum length of the total sentence (context + question) in tokens of each chunk passed to the\u001b[0m\n",
      "\u001b[0;34m                model. The context will be split in several chunks (using docStride as overlap) if needed.\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The number of answers to return (will be chosen by order of likelihood). Note that we return less than\u001b[0m\n",
      "\u001b[0;34m                topk answers if there are not enough options available within the context.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            Union[`QuestionAnsweringOutputElement`, List[`QuestionAnsweringOutputElement`]]:\u001b[0m\n",
      "\u001b[0;34m                When top_k is 1 or not provided, it returns a single `QuestionAnsweringOutputElement`.\u001b[0m\n",
      "\u001b[0;34m                When top_k is greater than 1, it returns a list of `QuestionAnsweringOutputElement`.\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.question_answering(question=\"What's my name?\", context=\"My name is Clara and I live in Berkeley.\")\u001b[0m\n",
      "\u001b[0;34m        QuestionAnsweringOutputElement(answer='Clara', end=16, score=0.9326565265655518, start=11)\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"align_to_words\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0malign_to_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"doc_stride\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc_stride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"handle_impossible_answer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhandle_impossible_answer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"max_answer_len\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_answer_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"max_question_len\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_question_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"max_seq_len\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"question-answering\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Parse the response as a single `QuestionAnsweringOutputElement` when top_k is 1 or not provided, or a list of `QuestionAnsweringOutputElement` to ensure backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuestionAnsweringOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msentence_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_sentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Compute the semantic similarity between a sentence and a list of other sentences by comparing their embeddings.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            sentence (`str`):\u001b[0m\n",
      "\u001b[0;34m                The main sentence to compare to others.\u001b[0m\n",
      "\u001b[0;34m            other_sentences (`List[str]`):\u001b[0m\n",
      "\u001b[0;34m                The list of sentences to compare to.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the conversational task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint. If not provided, the default recommended conversational model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[float]`: The embedding representing the input text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.sentence_similarity(\u001b[0m\n",
      "\u001b[0;34m        ...     \"Machine learning is so easy.\",\u001b[0m\n",
      "\u001b[0;34m        ...     other_sentences=[\u001b[0m\n",
      "\u001b[0;34m        ...         \"Deep learning is so straightforward.\",\u001b[0m\n",
      "\u001b[0;34m        ...         \"This is so difficult, like rocket science.\",\u001b[0m\n",
      "\u001b[0;34m        ...         \"I can't believe how much I struggled with this.\",\u001b[0m\n",
      "\u001b[0;34m        ...     ],\u001b[0m\n",
      "\u001b[0;34m        ... )\u001b[0m\n",
      "\u001b[0;34m        [0.7785726189613342, 0.45876261591911316, 0.2906220555305481]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"source_sentence\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentences\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mother_sentences\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentence-similarity\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0m_bytes_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0m_deprecate_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.29\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdeprecated_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcustom_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"The `parameters` argument is deprecated and will be removed in a future version. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"Provide individual parameters instead: `clean_up_tokenization_spaces`, `generate_parameters`, and `truncation`.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msummarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgenerate_parameters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SummarizationTruncationStrategy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSummarizationOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Generate a summary of a given text using a specified model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            text (`str`):\u001b[0m\n",
      "\u001b[0;34m                The input text to summarize.\u001b[0m\n",
      "\u001b[0;34m            parameters (`Dict[str, Any]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Additional parameters for summarization. Check out this [page](https://huggingface.co/docs/api-inference/detailed_parameters#summarization-task)\u001b[0m\n",
      "\u001b[0;34m                for more details.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. If not provided, the default recommended model for summarization will be used.\u001b[0m\n",
      "\u001b[0;34m            clean_up_tokenization_spaces (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to clean up the potential extra spaces in the text output.\u001b[0m\n",
      "\u001b[0;34m            generate_parameters (`Dict[str, Any]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Additional parametrization of the text generation algorithm.\u001b[0m\n",
      "\u001b[0;34m            truncation (`\"SummarizationTruncationStrategy\"`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The truncation strategy to use.\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            [`SummarizationOutput`]: The generated summary text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.summarization(\"The Eiffel tower...\")\u001b[0m\n",
      "\u001b[0;34m        SummarizationOutput(generated_text=\"The Eiffel tower is one of the most famous landmarks in the world....\")\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"clean_up_tokenization_spaces\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"generate_parameters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgenerate_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"truncation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mSummarizationOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtable_question_answering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Padding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTableQuestionAnsweringOutputElement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Retrieve the answer to a question from information given in a table.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            table (`str`):\u001b[0m\n",
      "\u001b[0;34m                A table of data represented as a dict of lists where entries are headers and the lists are all the\u001b[0m\n",
      "\u001b[0;34m                values, all lists must have the same size.\u001b[0m\n",
      "\u001b[0;34m            query (`str`):\u001b[0m\n",
      "\u001b[0;34m                The query in plain text that you want to ask the table.\u001b[0m\n",
      "\u001b[0;34m            model (`str`):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the table-question-answering task. Can be a model ID hosted on the Hugging Face\u001b[0m\n",
      "\u001b[0;34m                Hub or a URL to a deployed Inference Endpoint.\u001b[0m\n",
      "\u001b[0;34m            padding (`\"Padding\"`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Activates and controls padding.\u001b[0m\n",
      "\u001b[0;34m            sequential (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to do inference sequentially or as a batch. Batching is faster, but models like SQA require the\u001b[0m\n",
      "\u001b[0;34m                inference to be done sequentially to extract relations within sequences, given their conversational\u001b[0m\n",
      "\u001b[0;34m                nature.\u001b[0m\n",
      "\u001b[0;34m            truncation (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Activates and controls truncation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            [`TableQuestionAnsweringOutputElement`]: a table question answering output containing the answer, coordinates, cells and the aggregator used.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> query = \"How many stars does the transformers repository have?\"\u001b[0m\n",
      "\u001b[0;34m        >>> table = {\"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"], \"Stars\": [\"36542\", \"4512\", \"3934\"]}\u001b[0m\n",
      "\u001b[0;34m        >>> client.table_question_answering(table, query, model=\"google/tapas-base-finetuned-wtq\")\u001b[0m\n",
      "\u001b[0;34m        TableQuestionAnsweringOutputElement(answer='36542', coordinates=[[0, 1]], cells=['36542'], aggregator='AVERAGE')\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"sequential\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msequential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"truncation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"table\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"table-question-answering\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mTableQuestionAnsweringOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtabular_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Classifying a target category (a group) based on a set of attributes.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            table (`Dict[str, Any]`):\u001b[0m\n",
      "\u001b[0;34m                Set of attributes to classify.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the tabular classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint. If not provided, the default recommended tabular classification model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List`: a list of labels, one per row in the initial table.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> table = {\u001b[0m\n",
      "\u001b[0;34m        ...     \"fixed_acidity\": [\"7.4\", \"7.8\", \"10.3\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"volatile_acidity\": [\"0.7\", \"0.88\", \"0.32\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"citric_acid\": [\"0\", \"0\", \"0.45\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"residual_sugar\": [\"1.9\", \"2.6\", \"6.4\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"chlorides\": [\"0.076\", \"0.098\", \"0.073\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"free_sulfur_dioxide\": [\"11\", \"25\", \"5\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"total_sulfur_dioxide\": [\"34\", \"67\", \"13\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"density\": [\"0.9978\", \"0.9968\", \"0.9976\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"pH\": [\"3.51\", \"3.2\", \"3.23\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"sulphates\": [\"0.56\", \"0.68\", \"0.82\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"alcohol\": [\"9.4\", \"9.8\", \"12.6\"],\u001b[0m\n",
      "\u001b[0;34m        ... }\u001b[0m\n",
      "\u001b[0;34m        >>> client.tabular_classification(table=table, model=\"julien-c/wine-quality\")\u001b[0m\n",
      "\u001b[0;34m        [\"5\", \"5\", \"5\"]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tabular-classification\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0m_bytes_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtabular_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Predicting a numerical target value given a set of attributes/features in a table.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            table (`Dict[str, Any]`):\u001b[0m\n",
      "\u001b[0;34m                Set of attributes stored in a table. The attributes used to predict the target can be both numerical and categorical.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the tabular regression task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint. If not provided, the default recommended tabular regression model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List`: a list of predicted numerical target values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> table = {\u001b[0m\n",
      "\u001b[0;34m        ...     \"Height\": [\"11.52\", \"12.48\", \"12.3778\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"Length1\": [\"23.2\", \"24\", \"23.9\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"Length2\": [\"25.4\", \"26.3\", \"26.5\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"Length3\": [\"30\", \"31.2\", \"31.1\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"Species\": [\"Bream\", \"Bream\", \"Bream\"],\u001b[0m\n",
      "\u001b[0;34m        ...     \"Width\": [\"4.02\", \"4.3056\", \"4.6961\"],\u001b[0m\n",
      "\u001b[0;34m        ... }\u001b[0m\n",
      "\u001b[0;34m        >>> client.tabular_regression(table, model=\"scikit-learn/Fish-Weight\")\u001b[0m\n",
      "\u001b[0;34m        [110, 120, 130]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tabular-regression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0m_bytes_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtext_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TextClassificationOutputTransform\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextClassificationOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Perform text classification (e.g. sentiment-analysis) on the given text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            text (`str`):\u001b[0m\n",
      "\u001b[0;34m                A string to be classified.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the text classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint. If not provided, the default recommended text classification model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                When specified, limits the output to the top K most probable classes.\u001b[0m\n",
      "\u001b[0;34m            function_to_apply (`\"TextClassificationOutputTransform\"`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The function to apply to the model outputs in order to retrieve the scores.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[TextClassificationOutputElement]`: a list of [`TextClassificationOutputElement`] items containing the predicted label and associated probability.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.text_classification(\"I like you\")\u001b[0m\n",
      "\u001b[0;34m        [\u001b[0m\n",
      "\u001b[0;34m            TextClassificationOutputElement(label='POSITIVE', score=0.9998695850372314),\u001b[0m\n",
      "\u001b[0;34m            TextClassificationOutputElement(label='NEGATIVE', score=0.0001304351753788069),\u001b[0m\n",
      "\u001b[0;34m        ]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"function_to_apply\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-classification\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mTextClassificationOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore [return-value]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtext_generation\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdetails\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Parameters from `TextGenerationInputGenerateParameters` (maintained manually)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0madapter_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbest_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdecoder_input_details\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgrammar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextGenerationInputGrammarType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_full_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Deprecated, use `stop` instead\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_n_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtypical_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwatermark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtext_generation\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdetails\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Parameters from `TextGenerationInputGenerateParameters` (maintained manually)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0madapter_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbest_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdecoder_input_details\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgrammar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextGenerationInputGrammarType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_full_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Deprecated, use `stop` instead\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_n_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtypical_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwatermark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTextGenerationOutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtext_generation\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdetails\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Parameters from `TextGenerationInputGenerateParameters` (maintained manually)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0madapter_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbest_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdecoder_input_details\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgrammar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextGenerationInputGrammarType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_full_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Deprecated, use `stop` instead\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_n_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtypical_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwatermark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtext_generation\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdetails\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Parameters from `TextGenerationInputGenerateParameters` (maintained manually)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0madapter_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbest_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdecoder_input_details\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgrammar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextGenerationInputGrammarType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_full_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Deprecated, use `stop` instead\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_n_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtypical_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwatermark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextGenerationStreamOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtext_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdetails\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Parameters from `TextGenerationInputGenerateParameters` (maintained manually)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0madapter_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbest_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdecoder_input_details\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgrammar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextGenerationInputGrammarType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_full_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Deprecated, use `stop` instead\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_n_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtypical_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwatermark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextGenerationOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextGenerationStreamOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtext_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdetails\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Parameters from `TextGenerationInputGenerateParameters` (maintained manually)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0madapter_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbest_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdecoder_input_details\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgrammar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextGenerationInputGrammarType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_full_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Manual default value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Deprecated, use `stop` instead\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_n_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtypical_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwatermark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextGenerationOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextGenerationStreamOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Given a prompt, generate the following text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        API endpoint is supposed to run with the `text-generation-inference` backend (TGI). This backend is the\u001b[0m\n",
      "\u001b[0;34m        go-to solution to run large language models at scale. However, for some smaller models (e.g. \"gpt2\") the\u001b[0m\n",
      "\u001b[0;34m        default `transformers` + `api-inference` solution is still in use. Both approaches have very similar APIs, but\u001b[0m\n",
      "\u001b[0;34m        not exactly the same. This method is compatible with both approaches but some parameters are only available for\u001b[0m\n",
      "\u001b[0;34m        `text-generation-inference`. If some parameters are ignored, a warning message is triggered but the process\u001b[0m\n",
      "\u001b[0;34m        continues correctly.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        To learn more about the TGI project, please refer to https://github.com/huggingface/text-generation-inference.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        <Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        If you want to generate a response from chat messages, you should use the [`InferenceClient.chat_completion`] method.\u001b[0m\n",
      "\u001b[0;34m        It accepts a list of messages instead of a single text prompt and handles the chat templating for you.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        </Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            prompt (`str`):\u001b[0m\n",
      "\u001b[0;34m                Input text.\u001b[0m\n",
      "\u001b[0;34m            details (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                By default, text_generation returns a string. Pass `details=True` if you want a detailed output (tokens,\u001b[0m\n",
      "\u001b[0;34m                probabilities, seed, finish reason, etc.). Only available for models running on with the\u001b[0m\n",
      "\u001b[0;34m                `text-generation-inference` backend.\u001b[0m\n",
      "\u001b[0;34m            stream (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                By default, text_generation returns the full generated text. Pass `stream=True` if you want a stream of\u001b[0m\n",
      "\u001b[0;34m                tokens to be returned. Only available for models running on with the `text-generation-inference`\u001b[0m\n",
      "\u001b[0;34m                backend.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            adapter_id (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Lora adapter id.\u001b[0m\n",
      "\u001b[0;34m            best_of (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Generate best_of sequences and return the one if the highest token logprobs.\u001b[0m\n",
      "\u001b[0;34m            decoder_input_details (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Return the decoder input token logprobs and ids. You must set `details=True` as well for it to be taken\u001b[0m\n",
      "\u001b[0;34m                into account. Defaults to `False`.\u001b[0m\n",
      "\u001b[0;34m            do_sample (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Activate logits sampling\u001b[0m\n",
      "\u001b[0;34m            frequency_penalty (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in\u001b[0m\n",
      "\u001b[0;34m                the text so far, decreasing the model's likelihood to repeat the same line verbatim.\u001b[0m\n",
      "\u001b[0;34m            grammar ([`TextGenerationInputGrammarType`], *optional*):\u001b[0m\n",
      "\u001b[0;34m                Grammar constraints. Can be either a JSONSchema or a regex.\u001b[0m\n",
      "\u001b[0;34m            max_new_tokens (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Maximum number of generated tokens. Defaults to 100.\u001b[0m\n",
      "\u001b[0;34m            repetition_penalty (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The parameter for repetition penalty. 1.0 means no penalty. See [this\u001b[0m\n",
      "\u001b[0;34m                paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\u001b[0m\n",
      "\u001b[0;34m            return_full_text (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to prepend the prompt to the generated text\u001b[0m\n",
      "\u001b[0;34m            seed (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Random sampling seed\u001b[0m\n",
      "\u001b[0;34m            stop (`List[str]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Stop generating tokens if a member of `stop` is generated.\u001b[0m\n",
      "\u001b[0;34m            stop_sequences (`List[str]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Deprecated argument. Use `stop` instead.\u001b[0m\n",
      "\u001b[0;34m            temperature (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The value used to module the logits distribution.\u001b[0m\n",
      "\u001b[0;34m            top_n_tokens (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Return information about the `top_n_tokens` most likely tokens at each generation step, instead of\u001b[0m\n",
      "\u001b[0;34m                just the sampled token.\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional`):\u001b[0m\n",
      "\u001b[0;34m                The number of highest probability vocabulary tokens to keep for top-k-filtering.\u001b[0m\n",
      "\u001b[0;34m            top_p (`float`, *optional`):\u001b[0m\n",
      "\u001b[0;34m                If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or\u001b[0m\n",
      "\u001b[0;34m                higher are kept for generation.\u001b[0m\n",
      "\u001b[0;34m            truncate (`int`, *optional`):\u001b[0m\n",
      "\u001b[0;34m                Truncate inputs tokens to the given size.\u001b[0m\n",
      "\u001b[0;34m            typical_p (`float`, *optional`):\u001b[0m\n",
      "\u001b[0;34m                Typical Decoding mass\u001b[0m\n",
      "\u001b[0;34m                See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666) for more information\u001b[0m\n",
      "\u001b[0;34m            watermark (`bool`, *optional`):\u001b[0m\n",
      "\u001b[0;34m                Watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `Union[str, TextGenerationOutput, Iterable[str], Iterable[TextGenerationStreamOutput]]`:\u001b[0m\n",
      "\u001b[0;34m            Generated text returned from the server:\u001b[0m\n",
      "\u001b[0;34m            - if `stream=False` and `details=False`, the generated text is returned as a `str` (default)\u001b[0m\n",
      "\u001b[0;34m            - if `stream=True` and `details=False`, the generated text is returned token by token as a `Iterable[str]`\u001b[0m\n",
      "\u001b[0;34m            - if `stream=False` and `details=True`, the generated text is returned with more details as a [`~huggingface_hub.TextGenerationOutput`]\u001b[0m\n",
      "\u001b[0;34m            - if `details=True` and `stream=True`, the generated text is returned token by token as a iterable of [`~huggingface_hub.TextGenerationStreamOutput`]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            `ValidationError`:\u001b[0m\n",
      "\u001b[0;34m                If input values are not valid. No HTTP call is made to the server.\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        # Case 1: generate text\u001b[0m\n",
      "\u001b[0;34m        >>> client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12)\u001b[0m\n",
      "\u001b[0;34m        '100% open source and built to be easy to use.'\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        # Case 2: iterate over the generated tokens. Useful for large generation.\u001b[0m\n",
      "\u001b[0;34m        >>> for token in client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12, stream=True):\u001b[0m\n",
      "\u001b[0;34m        ...     print(token)\u001b[0m\n",
      "\u001b[0;34m        100\u001b[0m\n",
      "\u001b[0;34m        %\u001b[0m\n",
      "\u001b[0;34m        open\u001b[0m\n",
      "\u001b[0;34m        source\u001b[0m\n",
      "\u001b[0;34m        and\u001b[0m\n",
      "\u001b[0;34m        built\u001b[0m\n",
      "\u001b[0;34m        to\u001b[0m\n",
      "\u001b[0;34m        be\u001b[0m\n",
      "\u001b[0;34m        easy\u001b[0m\n",
      "\u001b[0;34m        to\u001b[0m\n",
      "\u001b[0;34m        use\u001b[0m\n",
      "\u001b[0;34m        .\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        # Case 3: get more details about the generation process.\u001b[0m\n",
      "\u001b[0;34m        >>> client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12, details=True)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationOutput(\u001b[0m\n",
      "\u001b[0;34m            generated_text='100% open source and built to be easy to use.',\u001b[0m\n",
      "\u001b[0;34m            details=TextGenerationDetails(\u001b[0m\n",
      "\u001b[0;34m                finish_reason='length',\u001b[0m\n",
      "\u001b[0;34m                generated_tokens=12,\u001b[0m\n",
      "\u001b[0;34m                seed=None,\u001b[0m\n",
      "\u001b[0;34m                prefill=[\u001b[0m\n",
      "\u001b[0;34m                    TextGenerationPrefillOutputToken(id=487, text='The', logprob=None),\u001b[0m\n",
      "\u001b[0;34m                    TextGenerationPrefillOutputToken(id=53789, text=' hugging', logprob=-13.171875),\u001b[0m\n",
      "\u001b[0;34m                    (...)\u001b[0m\n",
      "\u001b[0;34m                    TextGenerationPrefillOutputToken(id=204, text=' ', logprob=-7.0390625)\u001b[0m\n",
      "\u001b[0;34m                ],\u001b[0m\n",
      "\u001b[0;34m                tokens=[\u001b[0m\n",
      "\u001b[0;34m                    TokenElement(id=1425, text='100', logprob=-1.0175781, special=False),\u001b[0m\n",
      "\u001b[0;34m                    TokenElement(id=16, text='%', logprob=-0.0463562, special=False),\u001b[0m\n",
      "\u001b[0;34m                    (...)\u001b[0m\n",
      "\u001b[0;34m                    TokenElement(id=25, text='.', logprob=-0.5703125, special=False)\u001b[0m\n",
      "\u001b[0;34m                ],\u001b[0m\n",
      "\u001b[0;34m                best_of_sequences=None\u001b[0m\n",
      "\u001b[0;34m            )\u001b[0m\n",
      "\u001b[0;34m        )\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        # Case 4: iterate over the generated tokens with more details.\u001b[0m\n",
      "\u001b[0;34m        # Last object is more complete, containing the full generated text and the finish reason.\u001b[0m\n",
      "\u001b[0;34m        >>> for details in client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12, details=True, stream=True):\u001b[0m\n",
      "\u001b[0;34m        ...     print(details)\u001b[0m\n",
      "\u001b[0;34m        ...\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=1425, text='100', logprob=-1.0175781, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=16, text='%', logprob=-0.0463562, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=1314, text=' open', logprob=-1.3359375, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=3178, text=' source', logprob=-0.28100586, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=273, text=' and', logprob=-0.5961914, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=3426, text=' built', logprob=-1.9423828, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=271, text=' to', logprob=-1.4121094, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=314, text=' be', logprob=-1.5224609, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=1833, text=' easy', logprob=-2.1132812, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=271, text=' to', logprob=-0.08520508, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(id=745, text=' use', logprob=-0.39453125, special=False), generated_text=None, details=None)\u001b[0m\n",
      "\u001b[0;34m        TextGenerationStreamOutput(token=TokenElement(\u001b[0m\n",
      "\u001b[0;34m            id=25,\u001b[0m\n",
      "\u001b[0;34m            text='.',\u001b[0m\n",
      "\u001b[0;34m            logprob=-0.5703125,\u001b[0m\n",
      "\u001b[0;34m            special=False),\u001b[0m\n",
      "\u001b[0;34m            generated_text='100% open source and built to be easy to use.',\u001b[0m\n",
      "\u001b[0;34m            details=TextGenerationStreamOutputStreamDetails(finish_reason='length', generated_tokens=12, seed=None)\u001b[0m\n",
      "\u001b[0;34m        )\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        # Case 5: generate constrained output using grammar\u001b[0m\n",
      "\u001b[0;34m        >>> response = client.text_generation(\u001b[0m\n",
      "\u001b[0;34m        ...     prompt=\"I saw a puppy a cat and a raccoon during my bike ride in the park\",\u001b[0m\n",
      "\u001b[0;34m        ...     model=\"HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1\",\u001b[0m\n",
      "\u001b[0;34m        ...     max_new_tokens=100,\u001b[0m\n",
      "\u001b[0;34m        ...     repetition_penalty=1.3,\u001b[0m\n",
      "\u001b[0;34m        ...     grammar={\u001b[0m\n",
      "\u001b[0;34m        ...         \"type\": \"json\",\u001b[0m\n",
      "\u001b[0;34m        ...         \"value\": {\u001b[0m\n",
      "\u001b[0;34m        ...             \"properties\": {\u001b[0m\n",
      "\u001b[0;34m        ...                 \"location\": {\"type\": \"string\"},\u001b[0m\n",
      "\u001b[0;34m        ...                 \"activity\": {\"type\": \"string\"},\u001b[0m\n",
      "\u001b[0;34m        ...                 \"animals_seen\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 5},\u001b[0m\n",
      "\u001b[0;34m        ...                 \"animals\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\u001b[0m\n",
      "\u001b[0;34m        ...             },\u001b[0m\n",
      "\u001b[0;34m        ...             \"required\": [\"location\", \"activity\", \"animals_seen\", \"animals\"],\u001b[0m\n",
      "\u001b[0;34m        ...         },\u001b[0m\n",
      "\u001b[0;34m        ...     },\u001b[0m\n",
      "\u001b[0;34m        ... )\u001b[0m\n",
      "\u001b[0;34m        >>> json.loads(response)\u001b[0m\n",
      "\u001b[0;34m        {\u001b[0m\n",
      "\u001b[0;34m            \"activity\": \"bike riding\",\u001b[0m\n",
      "\u001b[0;34m            \"animals\": [\"puppy\", \"cat\", \"raccoon\"],\u001b[0m\n",
      "\u001b[0;34m            \"animals_seen\": 3,\u001b[0m\n",
      "\u001b[0;34m            \"location\": \"park\"\u001b[0m\n",
      "\u001b[0;34m        }\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mdecoder_input_details\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"`decoder_input_details=True` has been passed to the server but `details=False` is set meaning that\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" the output from the server will be truncated.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdecoder_input_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mstop_sequences\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"`stop_sequences` is a deprecated argument for `text_generation` task\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" and will be removed in version '0.28.0'. Use `stop` instead.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_sequences\u001b[0m  \u001b[0;31m# use deprecated arg if provided\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Build payload\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"adapter_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madapter_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"best_of\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_of\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"decoder_input_details\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdecoder_input_details\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"details\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"do_sample\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"frequency_penalty\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"grammar\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"max_new_tokens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"repetition_penalty\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"return_full_text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreturn_full_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"top_n_tokens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_n_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"top_p\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"truncate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"typical_p\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtypical_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"watermark\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwatermark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"parameters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Remove some parameters if not a TGI server\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0munsupported_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_unsupported_text_generation_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsupported_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# The server does not support some parameters\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# => means it is not a TGI server\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# => remove unsupported parameters and warn the user\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mignored_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munsupported_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mignored_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignored_parameters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"API endpoint/model for text-generation is not served via TGI. Ignoring following parameters:\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34mf\" {', '.join(ignored_parameters)}.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mUserWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"API endpoint/model for text-generation is not served via TGI. Parameter `details=True` will\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\" be ignored meaning only the generated text will be returned.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mUserWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"API endpoint/model for text-generation is not served via TGI. Cannot return output as a stream.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\" Please pass `stream=False` as input.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Handle errors separately for more precise error messages\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbytes_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_KWARGS_NOT_USED_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBadRequestError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0munused_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"' \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0m_set_unsupported_text_generation_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_generation\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdetails\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0madapter_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madapter_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mbest_of\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_of\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdecoder_input_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_details\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mgrammar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mreturn_full_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_full_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mtop_n_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mtypical_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtypical_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mwatermark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwatermark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mraise_text_generation_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Parse output\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0m_stream_text_generation_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bytes_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Data can be a single element (dict) or an iterable of dicts where we select the first element of.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mTextGenerationOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdetails\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generated_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtext_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mheight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwidth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mguidance_scale\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mscheduler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtarget_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextToImageTargetSize\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Generate an image based on a given text using a specified model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        <Tip warning={true}>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        You must have `PIL` installed if you want to work with images (`pip install Pillow`).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        </Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            prompt (`str`):\u001b[0m\n",
      "\u001b[0;34m                The prompt to generate an image from.\u001b[0m\n",
      "\u001b[0;34m            negative_prompt (`List[str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                One or several prompt to guide what NOT to include in image generation.\u001b[0m\n",
      "\u001b[0;34m            height (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The height in pixels of the image to generate.\u001b[0m\n",
      "\u001b[0;34m            width (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The width in pixels of the image to generate.\u001b[0m\n",
      "\u001b[0;34m            num_inference_steps (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\u001b[0m\n",
      "\u001b[0;34m                expense of slower inference.\u001b[0m\n",
      "\u001b[0;34m            guidance_scale (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                A higher guidance scale value encourages the model to generate images closely linked to the text\u001b[0m\n",
      "\u001b[0;34m                prompt, but values too high may cause saturation and other artifacts.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. If not provided, the default recommended text-to-image model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            scheduler (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Override the scheduler with a compatible one.\u001b[0m\n",
      "\u001b[0;34m            target_size (`TextToImageTargetSize`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The size in pixel of the output image\u001b[0m\n",
      "\u001b[0;34m            seed (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Seed for the random number generator.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `Image`: The generated image.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> image = client.text_to_image(\"An astronaut riding a horse on the moon.\")\u001b[0m\n",
      "\u001b[0;34m        >>> image.save(\"astronaut.png\")\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> image = client.text_to_image(\u001b[0m\n",
      "\u001b[0;34m        ...     \"An astronaut riding a horse on the moon.\",\u001b[0m\n",
      "\u001b[0;34m        ...     negative_prompt=\"low resolution, blurry\",\u001b[0m\n",
      "\u001b[0;34m        ...     model=\"stabilityai/stable-diffusion-2-1\",\u001b[0m\n",
      "\u001b[0;34m        ... )\u001b[0m\n",
      "\u001b[0;34m        >>> image.save(\"better_astronaut.png\")\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"negative_prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"height\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"width\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"num_inference_steps\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"guidance_scale\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mguidance_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"scheduler\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"target_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-to-image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0m_bytes_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtext_to_speech\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TextToSpeechEarlyStoppingEnum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mepsilon_cutoff\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0meta_cutoff\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnum_beam_groups\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnum_beams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpenalty_alpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtypical_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0muse_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Synthesize an audio of a voice pronouncing a given text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            text (`str`):\u001b[0m\n",
      "\u001b[0;34m                The text to synthesize.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. If not provided, the default recommended text-to-speech model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            do_sample (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to use sampling instead of greedy decoding when generating new tokens.\u001b[0m\n",
      "\u001b[0;34m            early_stopping (`Union[bool, \"TextToSpeechEarlyStoppingEnum\"]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Controls the stopping condition for beam-based methods.\u001b[0m\n",
      "\u001b[0;34m            epsilon_cutoff (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                If set to float strictly between 0 and 1, only tokens with a conditional probability greater than\u001b[0m\n",
      "\u001b[0;34m                epsilon_cutoff will be sampled. In the paper, suggested values range from 3e-4 to 9e-4, depending on\u001b[0m\n",
      "\u001b[0;34m                the size of the model. See [Truncation Sampling as Language Model\u001b[0m\n",
      "\u001b[0;34m                Desmoothing](https://hf.co/papers/2210.15191) for more details.\u001b[0m\n",
      "\u001b[0;34m            eta_cutoff (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to float strictly\u001b[0m\n",
      "\u001b[0;34m                between 0 and 1, a token is only considered if it is greater than either eta_cutoff or sqrt(eta_cutoff)\u001b[0m\n",
      "\u001b[0;34m                * exp(-entropy(softmax(next_token_logits))). The latter term is intuitively the expected next token\u001b[0m\n",
      "\u001b[0;34m                probability, scaled by sqrt(eta_cutoff). In the paper, suggested values range from 3e-4 to 2e-3,\u001b[0m\n",
      "\u001b[0;34m                depending on the size of the model. See [Truncation Sampling as Language Model\u001b[0m\n",
      "\u001b[0;34m                Desmoothing](https://hf.co/papers/2210.15191) for more details.\u001b[0m\n",
      "\u001b[0;34m            max_length (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The maximum length (in tokens) of the generated text, including the input.\u001b[0m\n",
      "\u001b[0;34m            max_new_tokens (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The maximum number of tokens to generate. Takes precedence over max_length.\u001b[0m\n",
      "\u001b[0;34m            min_length (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The minimum length (in tokens) of the generated text, including the input.\u001b[0m\n",
      "\u001b[0;34m            min_new_tokens (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The minimum number of tokens to generate. Takes precedence over min_length.\u001b[0m\n",
      "\u001b[0;34m            num_beam_groups (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Number of groups to divide num_beams into in order to ensure diversity among different groups of beams.\u001b[0m\n",
      "\u001b[0;34m                See [this paper](https://hf.co/papers/1610.02424) for more details.\u001b[0m\n",
      "\u001b[0;34m            num_beams (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Number of beams to use for beam search.\u001b[0m\n",
      "\u001b[0;34m            penalty_alpha (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The value balances the model confidence and the degeneration penalty in contrastive search decoding.\u001b[0m\n",
      "\u001b[0;34m            temperature (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The value used to modulate the next token probabilities.\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The number of highest probability vocabulary tokens to keep for top-k-filtering.\u001b[0m\n",
      "\u001b[0;34m            top_p (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to\u001b[0m\n",
      "\u001b[0;34m                top_p or higher are kept for generation.\u001b[0m\n",
      "\u001b[0;34m            typical_p (`float`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Local typicality measures how similar the conditional probability of predicting a target token next is\u001b[0m\n",
      "\u001b[0;34m                to the expected conditional probability of predicting a random token next, given the partial text\u001b[0m\n",
      "\u001b[0;34m                already generated. If set to float < 1, the smallest set of the most locally typical tokens with\u001b[0m\n",
      "\u001b[0;34m                probabilities that add up to typical_p or higher are kept for generation. See [this\u001b[0m\n",
      "\u001b[0;34m                paper](https://hf.co/papers/2202.00666) for more details.\u001b[0m\n",
      "\u001b[0;34m            use_cache (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether the model should use the past last key/values attentions to speed up decoding\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `bytes`: The generated audio.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from pathlib import Path\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> audio = client.text_to_speech(\"Hello world\")\u001b[0m\n",
      "\u001b[0;34m        >>> Path(\"hello_world.flac\").write_bytes(audio)\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"do_sample\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"early_stopping\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"epsilon_cutoff\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepsilon_cutoff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"eta_cutoff\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meta_cutoff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"max_new_tokens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"min_length\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"min_new_tokens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"num_beam_groups\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum_beam_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"num_beams\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"penalty_alpha\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"top_p\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"typical_p\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtypical_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-to-speech\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtoken_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0maggregation_strategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TokenClassificationAggregationStrategy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mignore_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTokenClassificationOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Perform token classification on the given text.\u001b[0m\n",
      "\u001b[0;34m        Usually used for sentence parsing, either grammatical, or Named Entity Recognition (NER) to understand keywords contained within text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            text (`str`):\u001b[0m\n",
      "\u001b[0;34m                A string to be classified.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the token classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint. If not provided, the default recommended token classification model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            aggregation_strategy (`\"TokenClassificationAggregationStrategy\"`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The strategy used to fuse tokens based on model predictions\u001b[0m\n",
      "\u001b[0;34m            ignore_labels (`List[str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                A list of labels to ignore\u001b[0m\n",
      "\u001b[0;34m            stride (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The number of overlapping tokens between chunks when splitting the input text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[TokenClassificationOutputElement]`: List of [`TokenClassificationOutputElement`] items containing the entity group, confidence score, word, start and end index.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.token_classification(\"My name is Sarah Jessica Parker but you can call me Jessica\")\u001b[0m\n",
      "\u001b[0;34m        [\u001b[0m\n",
      "\u001b[0;34m            TokenClassificationOutputElement(\u001b[0m\n",
      "\u001b[0;34m                entity_group='PER',\u001b[0m\n",
      "\u001b[0;34m                score=0.9971321225166321,\u001b[0m\n",
      "\u001b[0;34m                word='Sarah Jessica Parker',\u001b[0m\n",
      "\u001b[0;34m                start=11,\u001b[0m\n",
      "\u001b[0;34m                end=31,\u001b[0m\n",
      "\u001b[0;34m            ),\u001b[0m\n",
      "\u001b[0;34m            TokenClassificationOutputElement(\u001b[0m\n",
      "\u001b[0;34m                entity_group='PER',\u001b[0m\n",
      "\u001b[0;34m                score=0.9773476123809814,\u001b[0m\n",
      "\u001b[0;34m                word='Jessica',\u001b[0m\n",
      "\u001b[0;34m                start=52,\u001b[0m\n",
      "\u001b[0;34m                end=59,\u001b[0m\n",
      "\u001b[0;34m            )\u001b[0m\n",
      "\u001b[0;34m        ]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"aggregation_strategy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maggregation_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"ignore_labels\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mignore_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"stride\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"token-classification\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mTokenClassificationOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msrc_lang\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtgt_lang\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TranslationTruncationStrategy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgenerate_parameters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTranslationOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Convert text from one language to another.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Check out https://huggingface.co/tasks/translation for more information on how to choose the best model for\u001b[0m\n",
      "\u001b[0;34m        your specific use case. Source and target languages usually depend on the model.\u001b[0m\n",
      "\u001b[0;34m        However, it is possible to specify source and target languages for certain models. If you are working with one of these models,\u001b[0m\n",
      "\u001b[0;34m        you can use `src_lang` and `tgt_lang` arguments to pass the relevant information.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            text (`str`):\u001b[0m\n",
      "\u001b[0;34m                A string to be translated.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the translation task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint. If not provided, the default recommended translation model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            src_lang (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The source language of the text. Required for models that can translate from multiple languages.\u001b[0m\n",
      "\u001b[0;34m            tgt_lang (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Target language to translate to. Required for models that can translate to multiple languages.\u001b[0m\n",
      "\u001b[0;34m            clean_up_tokenization_spaces (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether to clean up the potential extra spaces in the text output.\u001b[0m\n",
      "\u001b[0;34m            truncation (`\"TranslationTruncationStrategy\"`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The truncation strategy to use.\u001b[0m\n",
      "\u001b[0;34m            generate_parameters (`Dict[str, Any]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Additional parametrization of the text generation algorithm.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            [`TranslationOutput`]: The generated translated text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m            `ValueError`:\u001b[0m\n",
      "\u001b[0;34m                If only one of the `src_lang` and `tgt_lang` arguments are provided.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.translation(\"My name is Wolfgang and I live in Berlin\")\u001b[0m\n",
      "\u001b[0;34m        'Mein Name ist Wolfgang und ich lebe in Berlin.'\u001b[0m\n",
      "\u001b[0;34m        >>> client.translation(\"My name is Wolfgang and I live in Berlin\", model=\"Helsinki-NLP/opus-mt-en-fr\")\u001b[0m\n",
      "\u001b[0;34m        TranslationOutput(translation_text='Je m'appelle Wolfgang et je vis  Berlin.')\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Specifying languages:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> client.translation(\"My name is Sarah Jessica Parker but you can call me Jessica\", model=\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"en_XX\", tgt_lang=\"fr_XX\")\u001b[0m\n",
      "\u001b[0;34m        \"Mon nom est Sarah Jessica Parker mais vous pouvez m'appeler Jessica\"\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Throw error if only one of `src_lang` and `tgt_lang` was given\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msrc_lang\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtgt_lang\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify `src_lang` without specifying `tgt_lang`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msrc_lang\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtgt_lang\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify `tgt_lang` without specifying `src_lang`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"src_lang\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"tgt_lang\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtgt_lang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"clean_up_tokenization_spaces\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"truncation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"generate_parameters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgenerate_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"translation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mTranslationOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mvisual_question_answering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVisualQuestionAnsweringOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Answering open-ended questions based on an image.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            image (`Union[str, Path, bytes, BinaryIO]`):\u001b[0m\n",
      "\u001b[0;34m                The input image for the context. It can be raw bytes, an image file, or a URL to an online image.\u001b[0m\n",
      "\u001b[0;34m            question (`str`):\u001b[0m\n",
      "\u001b[0;34m                Question to be answered.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for the visual question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to\u001b[0m\n",
      "\u001b[0;34m                a deployed Inference Endpoint. If not provided, the default recommended visual question answering model will be used.\u001b[0m\n",
      "\u001b[0;34m                Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The number of answers to return (will be chosen by order of likelihood). Note that we return less than\u001b[0m\n",
      "\u001b[0;34m                topk answers if there are not enough options available within the context.\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[VisualQuestionAnsweringOutputElement]`: a list of [`VisualQuestionAnsweringOutputElement`] items containing the predicted label and associated probability.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            `InferenceTimeoutError`:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.visual_question_answering(\u001b[0m\n",
      "\u001b[0;34m        ...     image=\"https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\",\u001b[0m\n",
      "\u001b[0;34m        ...     question=\"What is the animal doing?\"\u001b[0m\n",
      "\u001b[0;34m        ... )\u001b[0m\n",
      "\u001b[0;34m        [\u001b[0m\n",
      "\u001b[0;34m            VisualQuestionAnsweringOutputElement(score=0.778609573841095, answer='laying down'),\u001b[0m\n",
      "\u001b[0;34m            VisualQuestionAnsweringOutputElement(score=0.6957435607910156, answer='sitting'),\u001b[0m\n",
      "\u001b[0;34m        ]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_b64_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parameters\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"visual-question-answering\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mVisualQuestionAnsweringOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0m_deprecate_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.30.0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdeprecated_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcustom_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"`labels`has been renamed to `candidate_labels` and will be removed in huggingface_hub>=0.30.0.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mzero_shot_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# temporarily keeping it optional for backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmulti_label\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# deprecated argument\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mZeroShotClassificationOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Provide as input a text and a set of candidate labels to classify the input text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            text (`str`):\u001b[0m\n",
      "\u001b[0;34m                The input text to classify.\u001b[0m\n",
      "\u001b[0;34m            candidate_labels (`List[str]`):\u001b[0m\n",
      "\u001b[0;34m                The set of possible class labels to classify the text into.\u001b[0m\n",
      "\u001b[0;34m            labels (`List[str]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                (deprecated) List of strings. Each string is the verbalization of a possible label for the input text.\u001b[0m\n",
      "\u001b[0;34m            multi_label (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Whether multiple candidate labels can be true. If false, the scores are normalized such that the sum of\u001b[0m\n",
      "\u001b[0;34m                the label likelihoods for each sequence is 1. If true, the labels are considered independent and\u001b[0m\n",
      "\u001b[0;34m                probabilities are normalized for each candidate.\u001b[0m\n",
      "\u001b[0;34m            hypothesis_template (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The sentence used in conjunction with `candidate_labels` to attempt the text classification by\u001b[0m\n",
      "\u001b[0;34m                replacing the placeholder with the candidate labels.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. This parameter overrides the model defined at the instance level. If not provided, the default recommended zero-shot classification model will be used.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[ZeroShotClassificationOutputElement]`: List of [`ZeroShotClassificationOutputElement`] items containing the predicted labels and their confidence.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example with `multi_label=False`:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> text = (\u001b[0m\n",
      "\u001b[0;34m        ...     \"A new model offers an explanation for how the Galilean satellites formed around the solar system's\"\u001b[0m\n",
      "\u001b[0;34m        ...     \"largest world. Konstantin Batygin did not set out to solve one of the solar system's most puzzling\"\u001b[0m\n",
      "\u001b[0;34m        ...     \" mysteries when he went for a run up a hill in Nice, France.\"\u001b[0m\n",
      "\u001b[0;34m        ... )\u001b[0m\n",
      "\u001b[0;34m        >>> labels = [\"space & cosmos\", \"scientific discovery\", \"microbiology\", \"robots\", \"archeology\"]\u001b[0m\n",
      "\u001b[0;34m        >>> client.zero_shot_classification(text, labels)\u001b[0m\n",
      "\u001b[0;34m        [\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='scientific discovery', score=0.7961668968200684),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='space & cosmos', score=0.18570658564567566),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='microbiology', score=0.00730885099619627),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='archeology', score=0.006258360575884581),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='robots', score=0.004559356719255447),\u001b[0m\n",
      "\u001b[0;34m        ]\u001b[0m\n",
      "\u001b[0;34m        >>> client.zero_shot_classification(text, labels, multi_label=True)\u001b[0m\n",
      "\u001b[0;34m        [\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='scientific discovery', score=0.9829297661781311),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='space & cosmos', score=0.755190908908844),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='microbiology', score=0.0005462635890580714),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='archeology', score=0.00047131875180639327),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='robots', score=0.00030448526376858354),\u001b[0m\n",
      "\u001b[0;34m        ]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example with `multi_label=True` and a custom `hypothesis_template`:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.zero_shot_classification(\u001b[0m\n",
      "\u001b[0;34m        ...    text=\"I really like our dinner and I'm very happy. I don't like the weather though.\",\u001b[0m\n",
      "\u001b[0;34m        ...    labels=[\"positive\", \"negative\", \"pessimistic\", \"optimistic\"],\u001b[0m\n",
      "\u001b[0;34m        ...    multi_label=True,\u001b[0m\n",
      "\u001b[0;34m        ...    hypothesis_template=\"This text is {} towards the weather\"\u001b[0m\n",
      "\u001b[0;34m        ... )\u001b[0m\n",
      "\u001b[0;34m        [\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='negative', score=0.9231801629066467),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='pessimistic', score=0.8760990500450134),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='optimistic', score=0.0008674879791215062),\u001b[0m\n",
      "\u001b[0;34m            ZeroShotClassificationOutputElement(label='positive', score=0.0005250611575320363)\u001b[0m\n",
      "\u001b[0;34m        ]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# handle deprecation\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mcandidate_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"Cannot specify both `labels` and `candidate_labels`. Use `candidate_labels` instead.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcandidate_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mcandidate_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Must specify `candidate_labels`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"candidate_labels\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"multi_label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmulti_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"hypothesis_template\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"zero-shot-classification\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bytes_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mZeroShotClassificationOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0m_deprecate_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.30.0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdeprecated_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcustom_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"`labels`has been renamed to `candidate_labels` and will be removed in huggingface_hub>=0.30.0.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mzero_shot_image_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# temporarily keeping it optional for backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# deprecated argument\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mZeroShotImageClassificationOutputElement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Provide input image and text labels to predict text labels for the image.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            image (`Union[str, Path, bytes, BinaryIO]`):\u001b[0m\n",
      "\u001b[0;34m                The input image to caption. It can be raw bytes, an image file, or a URL to an online image.\u001b[0m\n",
      "\u001b[0;34m            candidate_labels (`List[str]`):\u001b[0m\n",
      "\u001b[0;34m                The candidate labels for this image\u001b[0m\n",
      "\u001b[0;34m            labels (`List[str]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                (deprecated) List of string possible labels. There must be at least 2 labels.\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. This parameter overrides the model defined at the instance level. If not provided, the default recommended zero-shot image classification model will be used.\u001b[0m\n",
      "\u001b[0;34m            hypothesis_template (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The sentence used in conjunction with `candidate_labels` to attempt the image classification by\u001b[0m\n",
      "\u001b[0;34m                replacing the placeholder with the candidate labels.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[ZeroShotImageClassificationOutputElement]`: List of [`ZeroShotImageClassificationOutputElement`] items containing the predicted labels and their confidence.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            [`InferenceTimeoutError`]:\u001b[0m\n",
      "\u001b[0;34m                If the model is unavailable or the request times out.\u001b[0m\n",
      "\u001b[0;34m            `HTTPError`:\u001b[0m\n",
      "\u001b[0;34m                If the request fails with an HTTP error status code other than HTTP 503.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> client.zero_shot_image_classification(\u001b[0m\n",
      "\u001b[0;34m        ...     \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\",\u001b[0m\n",
      "\u001b[0;34m        ...     labels=[\"dog\", \"cat\", \"horse\"],\u001b[0m\n",
      "\u001b[0;34m        ... )\u001b[0m\n",
      "\u001b[0;34m        [ZeroShotImageClassificationOutputElement(label='dog', score=0.956),...]\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# handle deprecation\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mcandidate_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"Cannot specify both `labels` and `candidate_labels`. Use `candidate_labels` instead.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcandidate_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mcandidate_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Must specify `candidate_labels`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Raise ValueError if input is less than 2 labels\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must specify at least 2 classes to compare.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"candidate_labels\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"hypothesis_template\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"zero-shot-image-classification\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mZeroShotImageClassificationOutputElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_resolve_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# If model is already a URL, ignore `task` and return directly\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# # If no model but task is set => fetch the recommended one for this task\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"You must specify at least a model (repo_id or URL) or a task, either when instantiating\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\" `InferenceClient` or when making a request.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recommended_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"Using recommended model {model} for task {task}. Note that it is\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34mf\" encouraged to explicitly set `model='{model}'` as the recommended\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" models list might get updated without prior notice.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Compute InferenceAPI url\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Feature-extraction and sentence-similarity are the only cases where we handle models with several tasks.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"{INFERENCE_ENDPOINT}/pipeline/{task}/{model}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"feature-extraction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentence-similarity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Otherwise, we use the default endpoint\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0;34mf\"{INFERENCE_ENDPOINT}/models/{model}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_recommended_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Get the model Hugging Face recommends for the input task.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            task (`str`):\u001b[0m\n",
      "\u001b[0;34m                The Hugging Face task to get which model Hugging Face recommends.\u001b[0m\n",
      "\u001b[0;34m                All available tasks can be found [here](https://huggingface.co/tasks).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `str`: Name of the model recommended for the input task.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises:\u001b[0m\n",
      "\u001b[0;34m            `ValueError`: If Hugging Face has no recommendation for the input task.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fetch_recommended_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"Task {task} has no recommended model. Please specify a model\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" explicitly. Visit https://huggingface.co/tasks for more info.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_endpoint_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Get information about the deployed endpoint.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        This endpoint is only available on endpoints powered by Text-Generation-Inference (TGI) or Text-Embedding-Inference (TEI).\u001b[0m\n",
      "\u001b[0;34m        Endpoints powered by `transformers` return an empty payload.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\u001b[0m\n",
      "\u001b[0;34m                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `Dict[str, Any]`: Information about the endpoint.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient(\"meta-llama/Meta-Llama-3-70B-Instruct\")\u001b[0m\n",
      "\u001b[0;34m        >>> client.get_endpoint_info()\u001b[0m\n",
      "\u001b[0;34m        {\u001b[0m\n",
      "\u001b[0;34m            'model_id': 'meta-llama/Meta-Llama-3-70B-Instruct',\u001b[0m\n",
      "\u001b[0;34m            'model_sha': None,\u001b[0m\n",
      "\u001b[0;34m            'model_dtype': 'torch.float16',\u001b[0m\n",
      "\u001b[0;34m            'model_device_type': 'cuda',\u001b[0m\n",
      "\u001b[0;34m            'model_pipeline_tag': None,\u001b[0m\n",
      "\u001b[0;34m            'max_concurrent_requests': 128,\u001b[0m\n",
      "\u001b[0;34m            'max_best_of': 2,\u001b[0m\n",
      "\u001b[0;34m            'max_stop_sequences': 4,\u001b[0m\n",
      "\u001b[0;34m            'max_input_length': 8191,\u001b[0m\n",
      "\u001b[0;34m            'max_total_tokens': 8192,\u001b[0m\n",
      "\u001b[0;34m            'waiting_served_ratio': 0.3,\u001b[0m\n",
      "\u001b[0;34m            'max_batch_total_tokens': 1259392,\u001b[0m\n",
      "\u001b[0;34m            'max_waiting_tokens': 20,\u001b[0m\n",
      "\u001b[0;34m            'max_batch_size': None,\u001b[0m\n",
      "\u001b[0;34m            'validation_workers': 32,\u001b[0m\n",
      "\u001b[0;34m            'max_client_batch_size': 4,\u001b[0m\n",
      "\u001b[0;34m            'version': '2.0.2',\u001b[0m\n",
      "\u001b[0;34m            'sha': 'dccab72549635c7eb5ddb17f43f0b7cdff07c214',\u001b[0m\n",
      "\u001b[0;34m            'docker_label': 'sha-dccab72'\u001b[0m\n",
      "\u001b[0;34m        }\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model id not provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/info\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{INFERENCE_ENDPOINT}/models/{model}/info\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mhealth_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Check the health of the deployed endpoint.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Health check is only available with Inference Endpoints powered by Text-Generation-Inference (TGI) or Text-Embedding-Inference (TEI).\u001b[0m\n",
      "\u001b[0;34m        For Inference API, please use [`InferenceClient.get_model_status`] instead.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                URL of the Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `bool`: True if everything is working fine.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient(\"https://jzgu0buei5.us-east-1.aws.endpoints.huggingface.cloud\")\u001b[0m\n",
      "\u001b[0;34m        >>> client.health_check()\u001b[0m\n",
      "\u001b[0;34m        True\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model id not provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Model must be an Inference Endpoint URL. For serverless Inference API, please use `InferenceClient.get_model_status`.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/health\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_model_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mModelStatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Get the status of a model hosted on the Inference API.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        <Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        This endpoint is mostly useful when you already know which model you want to use and want to check its\u001b[0m\n",
      "\u001b[0;34m        availability. If you want to discover already deployed models, you should rather use [`~InferenceClient.list_deployed_models`].\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        </Tip>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            model (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Identifier of the model for witch the status gonna be checked. If model is not provided,\u001b[0m\n",
      "\u001b[0;34m                the model associated with this instance of [`InferenceClient`] will be used. Only InferenceAPI service can be checked so the\u001b[0m\n",
      "\u001b[0;34m                identifier cannot be a URL.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            [`ModelStatus`]: An instance of ModelStatus dataclass, containing information,\u001b[0m\n",
      "\u001b[0;34m                         about the state of the model: load, state, compute type and framework.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Example:\u001b[0m\n",
      "\u001b[0;34m        ```py\u001b[0m\n",
      "\u001b[0;34m        >>> from huggingface_hub import InferenceClient\u001b[0m\n",
      "\u001b[0;34m        >>> client = InferenceClient()\u001b[0m\n",
      "\u001b[0;34m        >>> client.get_model_status(\"meta-llama/Meta-Llama-3-8B-Instruct\")\u001b[0m\n",
      "\u001b[0;34m        ModelStatus(loaded=True, state='Loaded', compute_type='gpu', framework='text-generation-inference')\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model id not provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model status is only available for Inference API endpoints.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{INFERENCE_ENDPOINT}/status/{model}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mModelStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mloaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcompute_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compute_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"framework\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"ProxyClientChat\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mProxyClientChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "search_tool = DuckDuckGoSearchTool()\n",
    "model = HfApiModel()\n",
    "\n",
    "# let's see the model connected to do the tasks\n",
    "model.client??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- {{ tool.name }}: {{ tool.description }}\\n    Takes inputs: {{tool.inputs}}\\n    Returns an output of type: {{tool.output_type}}\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[search_tool]\n",
    ")\n",
    "\n",
    "agent.tool_description_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Performs a duckduckgo web search based on your query (think a Google search) then returns the top search results.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the tool expected to do?\n",
    "search_tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\"></span>                                                                                                                                                                                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\"></span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\"></span> <span style=\"font-weight: bold\">What was the capital of France before Paris?</span>                                                                                                                                                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\"></span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\"></span>                                                                                                                                                                                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\"></span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\"> HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m\u001b[0m\u001b[38;2;212;183;2m\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m\u001b[0m\u001b[38;2;212;183;2m\u001b[0m\n",
       "\u001b[38;2;212;183;2m\u001b[0m                                                                                                                                                                                                      \u001b[38;2;212;183;2m\u001b[0m\n",
       "\u001b[38;2;212;183;2m\u001b[0m \u001b[1mWhat was the capital of France before Paris?\u001b[0m                                                                                                                                                         \u001b[38;2;212;183;2m\u001b[0m\n",
       "\u001b[38;2;212;183;2m\u001b[0m                                                                                                                                                                                                      \u001b[38;2;212;183;2m\u001b[0m\n",
       "\u001b[38;2;212;183;2m\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m\u001b[0m\u001b[38;2;212;183;2m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"font-weight: bold\">Executing this code:</span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">result </span><span style=\"color: #f92672; text-decoration-color: #f92672; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #f92672; text-decoration-color: #f92672; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"capital of France before Paris\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(result)</span><span style=\"background-color: #272822\">                                                                                                                                                                                       </span>  \n",
       "  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[1mExecuting this code:\u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcapital of France before Paris\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                                                                                       \u001b[0m  \n",
       "  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "## Search Results\n",
       "\n",
       "[List of capitals of France - Wikipedia](https://en.wikipedia.org/wiki/List_of_capitals_of_France)\n",
       "This is a chronological list of capitals of France. The capital of France has been Paris since its liberation in 1944. [1] Chronology. Tournai (before 486), current-day Belgium; Soissons (486-936) \n",
       "Laon (936-987) Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.\n",
       "\n",
       "[What was the capital of France before Paris? - StudyCountry.com](https://www.studycountry.com/wiki/what-was-the-capital-of-france-before-paris)\n",
       "Who moved the capital of France to Paris? The Frankish king Clovis I had taken Paris from the Gauls by 494 ce and later made his capital there. Under Hugh Capet (ruled 987-996) and the Capetian \n",
       "dynasty the preeminence of Paris was firmly established, and Paris became the political and cultural hub as modern France took shape.\n",
       "\n",
       "[What happened to France's other capitals? - The French History Podcast](https://www.thefrenchhistorypodcast.com/what-happened-to-frances-other-capitals/)\n",
       "This was a desperate gamble meant to conquer France before enough American reinforcements could land in the country and turn the tide of war. ... In a supreme twist of irony, de Gaulle's organization \n",
       "claimed Algiers was the (temporary) capital of France after 100 years of Paris being the capital of colonial Algeria. On 6 June 1944 the Allies ...\n",
       "\n",
       "[Capital of France - Simple English Wikipedia, the free encyclopedia](https://simple.wikipedia.org/wiki/Capital_of_France)\n",
       "The capital of France is Paris. [1] In the course of history, the national capital has been in many locations other than Paris. History ... Tournai (before 486) Soissons (486-ca. 900) Paris (900-1419)\n",
       "The residence of the kings of France, but they were consecrated at Reims.\n",
       "\n",
       "[List of capitals of France - Wikiwand](https://www.wikiwand.com/en/articles/List_of_capitals_of_France)\n",
       "Tournai (before 486), current-day Belgium; Soissons (486-936); Laon (936-987); Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.; Orlans (1108), one of \n",
       "the few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, Archbishop of Sens; from ...\n",
       "\n",
       "[Where was the capital of France moved to?](https://www.ncesc.com/geographic-faq/where-was-the-capital-of-france-moved-to/)\n",
       "The first capital of France was the city of Paris. It has been the capital since the 10th century. Metz. Pepin the Short constructed a palace in Aachen, the exact year is not known and Aachen became \n",
       "the Seat of the King in the 8th century. Where was the French capital before Paris? However, Paris has not been the only capital in the country's ...\n",
       "\n",
       "[Why was Lutetia renamed Paris? - Geographic FAQ Hub: Answers to ... - NCESC](https://www.ncesc.com/geographic-faq/why-was-lutetia-renamed-paris/)\n",
       "Before Paris, the capital of France was located in various cities throughout history. However, one notable city that served as the capital before Paris was Vichy, which was the de facto capital of \n",
       "France during the French State from 1940 to 1944.\n",
       "\n",
       "[Capital of France facts for kids - Kids encyclopedia](https://kids.kiddle.co/Capital_of_France)\n",
       "History List of capitals of France. Tournai (before 486); Soissons (486-ca. 900); Paris (900-1419) The residence of the kings of France, but they were consecrated at Reims.; Orlans (1108) One of the \n",
       "few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, archbishop of Sens and from 13 ...\n",
       "\n",
       "[Laon, medieval capital of France - Notes from Camelid Country](https://notesfromcamelidcountry.net/2017/08/18/laon-medieval-capital-of-france/)\n",
       "It's almost impossible to imagine today, but long before Paris became capital of France and went on to become one of the most famous cities on the planet, Laon, a little know and remarkably \n",
       "tourist-free town near the Belgian border, was the medieval capital of France. It gained this distinction during the reign of the Carolingian dynasty, the ...\n",
       "\n",
       "[What is the Capital of France? Paris - Countryaah.com](https://www.countryaah.com/france-faqs/)\n",
       "Other Cities That Served as Capital in France's History 1. Soissons (Before 508 - 508) Soissons was the capital of the Merovingian Kingdom of the Franks before Paris took over the role. King Clovis I \n",
       "established Soissons as the capital after his victory at the Battle of Soissons. In 508, Clovis moved the capital to Paris, where it has ...\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "## Search Results\n",
       "\n",
       "[List of capitals of France - Wikipedia](https://en.wikipedia.org/wiki/List_of_capitals_of_France)\n",
       "This is a chronological list of capitals of France. The capital of France has been Paris since its liberation in 1944. [1] Chronology. Tournai (before 486), current-day Belgium; Soissons (486-936) \n",
       "Laon (936-987) Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.\n",
       "\n",
       "[What was the capital of France before Paris? - StudyCountry.com](https://www.studycountry.com/wiki/what-was-the-capital-of-france-before-paris)\n",
       "Who moved the capital of France to Paris? The Frankish king Clovis I had taken Paris from the Gauls by 494 ce and later made his capital there. Under Hugh Capet (ruled 987-996) and the Capetian \n",
       "dynasty the preeminence of Paris was firmly established, and Paris became the political and cultural hub as modern France took shape.\n",
       "\n",
       "[What happened to France's other capitals? - The French History Podcast](https://www.thefrenchhistorypodcast.com/what-happened-to-frances-other-capitals/)\n",
       "This was a desperate gamble meant to conquer France before enough American reinforcements could land in the country and turn the tide of war. ... In a supreme twist of irony, de Gaulle's organization \n",
       "claimed Algiers was the (temporary) capital of France after 100 years of Paris being the capital of colonial Algeria. On 6 June 1944 the Allies ...\n",
       "\n",
       "[Capital of France - Simple English Wikipedia, the free encyclopedia](https://simple.wikipedia.org/wiki/Capital_of_France)\n",
       "The capital of France is Paris. [1] In the course of history, the national capital has been in many locations other than Paris. History ... Tournai (before 486) Soissons (486-ca. 900) Paris (900-1419)\n",
       "The residence of the kings of France, but they were consecrated at Reims.\n",
       "\n",
       "[List of capitals of France - Wikiwand](https://www.wikiwand.com/en/articles/List_of_capitals_of_France)\n",
       "Tournai (before 486), current-day Belgium; Soissons (486-936); Laon (936-987); Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.; Orlans (1108), one of \n",
       "the few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, Archbishop of Sens; from ...\n",
       "\n",
       "[Where was the capital of France moved to?](https://www.ncesc.com/geographic-faq/where-was-the-capital-of-france-moved-to/)\n",
       "The first capital of France was the city of Paris. It has been the capital since the 10th century. Metz. Pepin the Short constructed a palace in Aachen, the exact year is not known and Aachen became \n",
       "the Seat of the King in the 8th century. Where was the French capital before Paris? However, Paris has not been the only capital in the country's ...\n",
       "\n",
       "[Why was Lutetia renamed Paris? - Geographic FAQ Hub: Answers to ... - NCESC](https://www.ncesc.com/geographic-faq/why-was-lutetia-renamed-paris/)\n",
       "Before Paris, the capital of France was located in various cities throughout history. However, one notable city that served as the capital before Paris was Vichy, which was the de facto capital of \n",
       "France during the French State from 1940 to 1944.\n",
       "\n",
       "[Capital of France facts for kids - Kids encyclopedia](https://kids.kiddle.co/Capital_of_France)\n",
       "History List of capitals of France. Tournai (before 486); Soissons (486-ca. 900); Paris (900-1419) The residence of the kings of France, but they were consecrated at Reims.; Orlans (1108) One of the \n",
       "few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, archbishop of Sens and from 13 ...\n",
       "\n",
       "[Laon, medieval capital of France - Notes from Camelid Country](https://notesfromcamelidcountry.net/2017/08/18/laon-medieval-capital-of-france/)\n",
       "It's almost impossible to imagine today, but long before Paris became capital of France and went on to become one of the most famous cities on the planet, Laon, a little know and remarkably \n",
       "tourist-free town near the Belgian border, was the medieval capital of France. It gained this distinction during the reign of the Carolingian dynasty, the ...\n",
       "\n",
       "[What is the Capital of France? Paris - Countryaah.com](https://www.countryaah.com/france-faqs/)\n",
       "Other Cities That Served as Capital in France's History 1. Soissons (Before 508 - 508) Soissons was the capital of the Merovingian Kingdom of the Franks before Paris took over the role. King Clovis I \n",
       "established Soissons as the capital after his victory at the Battle of Soissons. In 508, Clovis moved the capital to Paris, where it has ...\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 0.97 seconds| Input tokens: 2,045 | Output tokens: 51]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 0: Duration 0.97 seconds| Input tokens: 2,045 | Output tokens: 51]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"font-weight: bold\">Executing this code:</span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">result </span><span style=\"color: #f92672; text-decoration-color: #f92672; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #f92672; text-decoration-color: #f92672; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Laon capital of France before Paris\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                                                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(result)</span><span style=\"background-color: #272822\">                                                                                                                                                                                       </span>  \n",
       "  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[1mExecuting this code:\u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLaon capital of France before Paris\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                                                                                       \u001b[0m  \n",
       "  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; text-decoration: underline\">https://lite.duckduckgo.com/lite/</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">202</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Ratelimit</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;4;31mhttps://lite.duckduckgo.com/lite/\u001b[0m\u001b[1;31m \u001b[0m\u001b[1;31m202\u001b[0m\u001b[1;31m Ratelimit\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 4.04 seconds| Input tokens: 5,433 | Output tokens: 132]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 4.04 seconds| Input tokens: 5,433 | Output tokens: 132]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"font-weight: bold\">Executing this code:</span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">result </span><span style=\"color: #f92672; text-decoration-color: #f92672; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #f92672; text-decoration-color: #f92672; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"capital of France before 987 AD\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                                                        </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(result)</span><span style=\"background-color: #272822\">                                                                                                                                                                                       </span>  \n",
       "  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[1mExecuting this code:\u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcapital of France before 987 AD\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                                        \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                                                                                       \u001b[0m  \n",
       "  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "## Search Results\n",
       "\n",
       "[List of capitals of France - Wikipedia](https://en.wikipedia.org/wiki/List_of_capitals_of_France)\n",
       "The capital of France has been Paris since its liberation in 1944. [1] Chronology. Tournai (before 486), current-day Belgium; Soissons (486-936) Laon (936-987) Paris (987-1419), the residence of the \n",
       "Kings of France, although they were consecrated at Reims.\n",
       "\n",
       "[Laon, medieval capital of France - Notes from Camelid Country](https://notesfromcamelidcountry.net/2017/08/18/laon-medieval-capital-of-france/)\n",
       "It's almost impossible to imagine today, but long before Paris became capital of France and went on to become one of the most famous cities on the planet, Laon, a little know and remarkably \n",
       "tourist-free town near the Belgian border, was the medieval capital of France. ... Power shifted to Paris in 987 AD when Hugues Capet claimed the crown ...\n",
       "\n",
       "[France in the Middle Ages - Wikipedia](https://en.wikipedia.org/wiki/France_in_the_Middle_Ages)\n",
       "At the end of the Middle Ages, France was the most populous region [clarification needed] in Europehaving overtaken Spain and Italy by 1340. [2] In the 14th century, before the arrival of the Black \n",
       "Death, the total population of the area covered by modern-day France has been estimated at 16 million. [3] The population of Paris is controversial. [4] ...\n",
       "\n",
       "[What was the capital of France before Paris? - StudyCountry.com](https://www.studycountry.com/wiki/what-was-the-capital-of-france-before-paris)\n",
       "What was the original capital of France? Christianity arrived in the 2nd century AD, and the 5th century saw the end of Roman rule with the arrival of the Franks. In 508 AD, Clovis I, the Frankish \n",
       "king, united Gaul as a kingdom and established Paris as the capital, naming it in honor of the original Parisii tribe.\n",
       "\n",
       "[Kingdom of France - Wikipedia](https://en.wikipedia.org/wiki/Kingdom_of_France)\n",
       "The Kingdom of France is the historiographical name or umbrella term given to various political entities of France in the medieval and early modern period. It was one of the most powerful states in \n",
       "Europe from the High Middle Ages to 1848 during its dissolution. It was also an early colonial power, with colonies in Asia and Africa, and the largest being New France in North America centred ...\n",
       "\n",
       "[What happened to France's other capitals? - The French History Podcast](https://www.thefrenchhistorypodcast.com/what-happened-to-frances-other-capitals/)\n",
       "However, in France's long history other cities have taken that role. In the modern period it's very clear which cities are the capital as they hold the permanent seat of government. Before that, the \n",
       "capital was usually wherever the king most commonly held court. When Hugh Capet became king in 987 he ruled France from his powerbase in the north.\n",
       "\n",
       "[How did France get its name? - Geographic Pedia](https://www.ncesc.com/geographic-pedia/how-did-france-get-its-name/)\n",
       "France Before its Name. ... Gaul was also referred to as Frankia when it was first conquered by the Franks in the 5th century AD. The Franks, a Germanic tribe, bestowed their name upon the region, \n",
       "eventually leading to the name \"France.\" ... In 987 A.D., Paris officially became the capital of France, solidifying its place in history. French ...\n",
       "\n",
       "[When did paris first become the capital of france?](https://www.reddit.com/r/AskHistorians/comments/18yz331/when_did_paris_first_become_the_capital_of_france/)\n",
       "When did paris first become the capital of france? Okay having some articals online saying it was in 987 but i thought it was the capital during Charles the balds reign am I misremembering or is the \n",
       "internet oversimplifying things? ... Please read the rules before participating, as we remove all comments which break the rules. ...\n",
       "\n",
       "[Kingdom of West Francia Timeline - World History Encyclopedia](https://www.worldhistory.org/timeline/Kingdom_of_West_Francia/)\n",
       "The Kingdom of West Francia (843-987 CE, also known as The Kingdom of the West Franks) was the region of Western Europe that formed the western part of the Carolingian Empire of Charlemagne (Holy \n",
       "Roman Emperor 800-814 CE) known as Francia or the Kingdom of the Franks. ... Birth of Charles the Simple's and Eadgifu's son Louis (later Louis IV of ...\n",
       "\n",
       "[The birth of the french capital - History of Paris city](http://www.paris-city.fr/GB/paris-city/au-fil-du-temps/histoire.php)\n",
       "BIRTH OF A CAPITAL Discover the history of Paris through the centuries Paris was founded in the 3rd century B.C. on le de la Cit by a community of Celts. They were a group of tribal fishermen \n",
       "called the Parisii who, pushed by emigration towards the banks of the Seine, made a permanent settlement there and profited from the area's fertility and temperate climate.\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "## Search Results\n",
       "\n",
       "[List of capitals of France - Wikipedia](https://en.wikipedia.org/wiki/List_of_capitals_of_France)\n",
       "The capital of France has been Paris since its liberation in 1944. [1] Chronology. Tournai (before 486), current-day Belgium; Soissons (486-936) Laon (936-987) Paris (987-1419), the residence of the \n",
       "Kings of France, although they were consecrated at Reims.\n",
       "\n",
       "[Laon, medieval capital of France - Notes from Camelid Country](https://notesfromcamelidcountry.net/2017/08/18/laon-medieval-capital-of-france/)\n",
       "It's almost impossible to imagine today, but long before Paris became capital of France and went on to become one of the most famous cities on the planet, Laon, a little know and remarkably \n",
       "tourist-free town near the Belgian border, was the medieval capital of France. ... Power shifted to Paris in 987 AD when Hugues Capet claimed the crown ...\n",
       "\n",
       "[France in the Middle Ages - Wikipedia](https://en.wikipedia.org/wiki/France_in_the_Middle_Ages)\n",
       "At the end of the Middle Ages, France was the most populous region [clarification needed] in Europehaving overtaken Spain and Italy by 1340. [2] In the 14th century, before the arrival of the Black \n",
       "Death, the total population of the area covered by modern-day France has been estimated at 16 million. [3] The population of Paris is controversial. [4] ...\n",
       "\n",
       "[What was the capital of France before Paris? - StudyCountry.com](https://www.studycountry.com/wiki/what-was-the-capital-of-france-before-paris)\n",
       "What was the original capital of France? Christianity arrived in the 2nd century AD, and the 5th century saw the end of Roman rule with the arrival of the Franks. In 508 AD, Clovis I, the Frankish \n",
       "king, united Gaul as a kingdom and established Paris as the capital, naming it in honor of the original Parisii tribe.\n",
       "\n",
       "[Kingdom of France - Wikipedia](https://en.wikipedia.org/wiki/Kingdom_of_France)\n",
       "The Kingdom of France is the historiographical name or umbrella term given to various political entities of France in the medieval and early modern period. It was one of the most powerful states in \n",
       "Europe from the High Middle Ages to 1848 during its dissolution. It was also an early colonial power, with colonies in Asia and Africa, and the largest being New France in North America centred ...\n",
       "\n",
       "[What happened to France's other capitals? - The French History Podcast](https://www.thefrenchhistorypodcast.com/what-happened-to-frances-other-capitals/)\n",
       "However, in France's long history other cities have taken that role. In the modern period it's very clear which cities are the capital as they hold the permanent seat of government. Before that, the \n",
       "capital was usually wherever the king most commonly held court. When Hugh Capet became king in 987 he ruled France from his powerbase in the north.\n",
       "\n",
       "[How did France get its name? - Geographic Pedia](https://www.ncesc.com/geographic-pedia/how-did-france-get-its-name/)\n",
       "France Before its Name. ... Gaul was also referred to as Frankia when it was first conquered by the Franks in the 5th century AD. The Franks, a Germanic tribe, bestowed their name upon the region, \n",
       "eventually leading to the name \"France.\" ... In 987 A.D., Paris officially became the capital of France, solidifying its place in history. French ...\n",
       "\n",
       "[When did paris first become the capital of france?](https://www.reddit.com/r/AskHistorians/comments/18yz331/when_did_paris_first_become_the_capital_of_france/)\n",
       "When did paris first become the capital of france? Okay having some articals online saying it was in 987 but i thought it was the capital during Charles the balds reign am I misremembering or is the \n",
       "internet oversimplifying things? ... Please read the rules before participating, as we remove all comments which break the rules. ...\n",
       "\n",
       "[Kingdom of West Francia Timeline - World History Encyclopedia](https://www.worldhistory.org/timeline/Kingdom_of_West_Francia/)\n",
       "The Kingdom of West Francia (843-987 CE, also known as The Kingdom of the West Franks) was the region of Western Europe that formed the western part of the Carolingian Empire of Charlemagne (Holy \n",
       "Roman Emperor 800-814 CE) known as Francia or the Kingdom of the Franks. ... Birth of Charles the Simple's and Eadgifu's son Louis (later Louis IV of ...\n",
       "\n",
       "[The birth of the french capital - History of Paris city](http://www.paris-city.fr/GB/paris-city/au-fil-du-temps/histoire.php)\n",
       "BIRTH OF A CAPITAL Discover the history of Paris through the centuries Paris was founded in the 3rd century B.C. on le de la Cit by a community of Celts. They were a group of tribal fishermen \n",
       "called the Parisii who, pushed by emigration towards the banks of the Seine, made a permanent settlement there and profited from the area's fertility and temperate climate.\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.62 seconds| Input tokens: 9,019 | Output tokens: 204]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.62 seconds| Input tokens: 9,019 | Output tokens: 204]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"font-weight: bold\">Executing this code:</span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Laon\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                                                                                                </span>  \n",
       "  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[1mExecuting this code:\u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLaon\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                                                                                \u001b[0m  \n",
       "  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: Laon</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mOut - Final answer: Laon\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.12 seconds| Input tokens: 13,908 | Output tokens: 304]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 3.12 seconds| Input tokens: 13,908 | Output tokens: 304]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = agent.run(\n",
    "    \"What was the capital of France before Paris?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemPromptStep(system_prompt='You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.\\nTo do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.\\nTo solve the task, you must plan forward to proceed in a series of steps, in a cycle of \\'Thought:\\', \\'Code:\\', and \\'Observation:\\' sequences.\\n\\nAt each step, in the \\'Thought:\\' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.\\nThen in the \\'Code:\\' sequence, you should write the code in simple Python. The code sequence must end with \\'<end_code>\\' sequence.\\nDuring each intermediate step, you can use \\'print()\\' to save whatever important information you will then need.\\nThese print outputs will then appear in the \\'Observation:\\' field, which will be available as input for the next step.\\nIn the end you have to return a final answer using the `final_answer` tool.\\n\\nHere are a few examples using notional tools:\\n---\\nTask: \"Generate an image of the oldest person in this document.\"\\n\\nThought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\\nCode:\\n```py\\nanswer = document_qa(document=document, question=\"Who is the oldest person mentioned?\")\\nprint(answer)\\n```<end_code>\\nObservation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\\n\\nThought: I will now generate an image showcasing the oldest person.\\nCode:\\n```py\\nimage = image_generator(\"A portrait of John Doe, a 55-year-old man living in Canada.\")\\nfinal_answer(image)\\n```<end_code>\\n\\n---\\nTask: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\\n\\nThought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool\\nCode:\\n```py\\nresult = 5 + 3 + 1294.678\\nfinal_answer(result)\\n```<end_code>\\n\\n---\\nTask:\\n\"Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.\\nYou have been provided with these additional arguments, that you can access using the keys as variables in your python code:\\n{\\'question\\': \\'Quel est l\\'animal sur l\\'image?\\', \\'image\\': \\'path/to/image.jpg\\'}\"\\n\\nThought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.\\nCode:\\n```py\\ntranslated_question = translator(question=question, src_lang=\"French\", tgt_lang=\"English\")\\nprint(f\"The translated question is {translated_question}.\")\\nanswer = image_qa(image=image, question=translated_question)\\nfinal_answer(f\"The answer is {answer}\")\\n```<end_code>\\n\\n---\\nTask:\\nIn a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.\\nWhat does he say was the consequence of Einstein learning too much math on his creativity, in one word?\\n\\nThought: I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.\\nCode:\\n```py\\npages = search(query=\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\")\\nprint(pages)\\n```<end_code>\\nObservation:\\nNo result found for query \"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\".\\n\\nThought: The query was maybe too restrictive and did not find any results. Let\\'s try again with a broader query.\\nCode:\\n```py\\npages = search(query=\"1979 interview Stanislaus Ulam\")\\nprint(pages)\\n```<end_code>\\nObservation:\\nFound 6 pages:\\n[Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)\\n\\n[Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)\\n\\n(truncated)\\n\\nThought: I will read the first 2 pages to know more.\\nCode:\\n```py\\nfor url in [\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/\", \"https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/\"]:\\n    whole_page = visit_webpage(url)\\n    print(whole_page)\\n    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Print separator between pages\\n```<end_code>\\nObservation:\\nManhattan Project Locations:\\nLos Alamos, NM\\nStanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at\\n(truncated)\\n\\nThought: I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: \"He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity.\" Let\\'s answer in one word.\\nCode:\\n```py\\nfinal_answer(\"diminished\")\\n```<end_code>\\n\\n---\\nTask: \"Which city has the highest population: Guangzhou or Shanghai?\"\\n\\nThought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\\nCode:\\n```py\\nfor city in [\"Guangzhou\", \"Shanghai\"]:\\n    print(f\"Population {city}:\", search(f\"{city} population\")\\n```<end_code>\\nObservation:\\nPopulation Guangzhou: [\\'Guangzhou has a population of 15 million inhabitants as of 2021.\\']\\nPopulation Shanghai: \\'26 million (2019)\\'\\n\\nThought: Now I know that Shanghai has the highest population.\\nCode:\\n```py\\nfinal_answer(\"Shanghai\")\\n```<end_code>\\n\\n---\\nTask: \"What is the current age of the pope, raised to the power 0.36?\"\\n\\nThought: I will use the tool `wiki` to get the age of the pope, and confirm that with a web search.\\nCode:\\n```py\\npope_age_wiki = wiki(query=\"current pope age\")\\nprint(\"Pope age as per wikipedia:\", pope_age_wiki)\\npope_age_search = web_search(query=\"current pope age\")\\nprint(\"Pope age as per google search:\", pope_age_search)\\n```<end_code>\\nObservation:\\nPope age: \"The pope Francis is currently 88 years old.\"\\n\\nThought: I know that the pope is 88 years old. Let\\'s compute the result using python code.\\nCode:\\n```py\\npope_current_age = 88 ** 0.36\\nfinal_answer(pope_current_age)\\n```<end_code>\\n\\nAbove example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools:\\n\\n\\n- web_search: Performs a duckduckgo web search based on your query (think a Google search) then returns the top search results.\\n    Takes inputs: {\\'query\\': {\\'type\\': \\'string\\', \\'description\\': \\'The search query to perform.\\'}}\\n    Returns an output of type: string\\n\\n- final_answer: Provides a final answer to the given problem.\\n    Takes inputs: {\\'answer\\': {\\'type\\': \\'any\\', \\'description\\': \\'The final answer to the problem\\'}}\\n    Returns an output of type: any\\n\\n\\n\\nHere are the rules you should always follow to solve your task:\\n1. Always provide a \\'Thought:\\' sequence, and a \\'Code:\\n```py\\' sequence ending with \\'```<end_code>\\' sequence, else you will fail.\\n2. Use only variables that you have defined!\\n3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in \\'answer = wiki({\\'query\\': \"What is the place where James Bond lives?\"})\\', but use the arguments directly as in \\'answer = wiki(query=\"What is the place where James Bond lives?\")\\'.\\n4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.\\n5. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.\\n6. Don\\'t name any new variable with the same name as a tool: for instance don\\'t name a variable \\'final_answer\\'.\\n7. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.\\n8. You can use imports in your code, but only from the following list of modules: {{authorized_imports}}\\n9. The state persists between code executions: so if in one step you\\'ve created variables or imported modules, these will all persist.\\n10. Don\\'t give up! You\\'re in charge of solving the task, not providing directions to solve it.\\n\\nNow Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\\n'),\n",
       " TaskStep(task='What was the capital of France before Paris?'),\n",
       " ActionStep(agent_memory=[{'role': <MessageRole.SYSTEM: 'system'>, 'content': 'You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.\\nTo do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.\\nTo solve the task, you must plan forward to proceed in a series of steps, in a cycle of \\'Thought:\\', \\'Code:\\', and \\'Observation:\\' sequences.\\n\\nAt each step, in the \\'Thought:\\' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.\\nThen in the \\'Code:\\' sequence, you should write the code in simple Python. The code sequence must end with \\'<end_code>\\' sequence.\\nDuring each intermediate step, you can use \\'print()\\' to save whatever important information you will then need.\\nThese print outputs will then appear in the \\'Observation:\\' field, which will be available as input for the next step.\\nIn the end you have to return a final answer using the `final_answer` tool.\\n\\nHere are a few examples using notional tools:\\n---\\nTask: \"Generate an image of the oldest person in this document.\"\\n\\nThought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\\nCode:\\n```py\\nanswer = document_qa(document=document, question=\"Who is the oldest person mentioned?\")\\nprint(answer)\\n```<end_code>\\nObservation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\\n\\nThought: I will now generate an image showcasing the oldest person.\\nCode:\\n```py\\nimage = image_generator(\"A portrait of John Doe, a 55-year-old man living in Canada.\")\\nfinal_answer(image)\\n```<end_code>\\n\\n---\\nTask: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\\n\\nThought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool\\nCode:\\n```py\\nresult = 5 + 3 + 1294.678\\nfinal_answer(result)\\n```<end_code>\\n\\n---\\nTask:\\n\"Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.\\nYou have been provided with these additional arguments, that you can access using the keys as variables in your python code:\\n{\\'question\\': \\'Quel est l\\'animal sur l\\'image?\\', \\'image\\': \\'path/to/image.jpg\\'}\"\\n\\nThought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.\\nCode:\\n```py\\ntranslated_question = translator(question=question, src_lang=\"French\", tgt_lang=\"English\")\\nprint(f\"The translated question is {translated_question}.\")\\nanswer = image_qa(image=image, question=translated_question)\\nfinal_answer(f\"The answer is {answer}\")\\n```<end_code>\\n\\n---\\nTask:\\nIn a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.\\nWhat does he say was the consequence of Einstein learning too much math on his creativity, in one word?\\n\\nThought: I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.\\nCode:\\n```py\\npages = search(query=\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\")\\nprint(pages)\\n```<end_code>\\nObservation:\\nNo result found for query \"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\".\\n\\nThought: The query was maybe too restrictive and did not find any results. Let\\'s try again with a broader query.\\nCode:\\n```py\\npages = search(query=\"1979 interview Stanislaus Ulam\")\\nprint(pages)\\n```<end_code>\\nObservation:\\nFound 6 pages:\\n[Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)\\n\\n[Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)\\n\\n(truncated)\\n\\nThought: I will read the first 2 pages to know more.\\nCode:\\n```py\\nfor url in [\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/\", \"https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/\"]:\\n    whole_page = visit_webpage(url)\\n    print(whole_page)\\n    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Print separator between pages\\n```<end_code>\\nObservation:\\nManhattan Project Locations:\\nLos Alamos, NM\\nStanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at\\n(truncated)\\n\\nThought: I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: \"He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity.\" Let\\'s answer in one word.\\nCode:\\n```py\\nfinal_answer(\"diminished\")\\n```<end_code>\\n\\n---\\nTask: \"Which city has the highest population: Guangzhou or Shanghai?\"\\n\\nThought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\\nCode:\\n```py\\nfor city in [\"Guangzhou\", \"Shanghai\"]:\\n    print(f\"Population {city}:\", search(f\"{city} population\")\\n```<end_code>\\nObservation:\\nPopulation Guangzhou: [\\'Guangzhou has a population of 15 million inhabitants as of 2021.\\']\\nPopulation Shanghai: \\'26 million (2019)\\'\\n\\nThought: Now I know that Shanghai has the highest population.\\nCode:\\n```py\\nfinal_answer(\"Shanghai\")\\n```<end_code>\\n\\n---\\nTask: \"What is the current age of the pope, raised to the power 0.36?\"\\n\\nThought: I will use the tool `wiki` to get the age of the pope, and confirm that with a web search.\\nCode:\\n```py\\npope_age_wiki = wiki(query=\"current pope age\")\\nprint(\"Pope age as per wikipedia:\", pope_age_wiki)\\npope_age_search = web_search(query=\"current pope age\")\\nprint(\"Pope age as per google search:\", pope_age_search)\\n```<end_code>\\nObservation:\\nPope age: \"The pope Francis is currently 88 years old.\"\\n\\nThought: I know that the pope is 88 years old. Let\\'s compute the result using python code.\\nCode:\\n```py\\npope_current_age = 88 ** 0.36\\nfinal_answer(pope_current_age)\\n```<end_code>\\n\\nAbove example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools:\\n\\n\\n- web_search: Performs a duckduckgo web search based on your query (think a Google search) then returns the top search results.\\n    Takes inputs: {\\'query\\': {\\'type\\': \\'string\\', \\'description\\': \\'The search query to perform.\\'}}\\n    Returns an output of type: string\\n\\n- final_answer: Provides a final answer to the given problem.\\n    Takes inputs: {\\'answer\\': {\\'type\\': \\'any\\', \\'description\\': \\'The final answer to the problem\\'}}\\n    Returns an output of type: any\\n\\n\\n\\nHere are the rules you should always follow to solve your task:\\n1. Always provide a \\'Thought:\\' sequence, and a \\'Code:\\n```py\\' sequence ending with \\'```<end_code>\\' sequence, else you will fail.\\n2. Use only variables that you have defined!\\n3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in \\'answer = wiki({\\'query\\': \"What is the place where James Bond lives?\"})\\', but use the arguments directly as in \\'answer = wiki(query=\"What is the place where James Bond lives?\")\\'.\\n4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.\\n5. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.\\n6. Don\\'t name any new variable with the same name as a tool: for instance don\\'t name a variable \\'final_answer\\'.\\n7. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.\\n8. You can use imports in your code, but only from the following list of modules: {{authorized_imports}}\\n9. The state persists between code executions: so if in one step you\\'ve created variables or imported modules, these will all persist.\\n10. Don\\'t give up! You\\'re in charge of solving the task, not providing directions to solve it.\\n\\nNow Begin! If you solve the task correctly, you will receive a reward of $1,000,000.'}, {'role': <MessageRole.USER: 'user'>, 'content': 'New task:\\nWhat was the capital of France before Paris?'}], tool_calls=[ToolCall(name='python_interpreter', arguments='result = web_search(query=\"capital of France before Paris\")\\nprint(result)', id='call_2')], start_time=1737104844.9939232, end_time=1737104845.9656925, step=0, error=None, duration=0.9717693328857422, llm_output='Thought: To determine the capital of France before Paris, I\\'ll perform a web search for historical information about the capital of France.\\n\\nCode:\\n```py\\nresult = web_search(query=\"capital of France before Paris\")\\nprint(result)\\n```<end_code>', observations=\"Execution logs:\\n## Search Results\\n\\n[List of capitals of France - Wikipedia](https://en.wikipedia.org/wiki/List_of_capitals_of_France)\\nThis is a chronological list of capitals of France. The capital of France has been Paris since its liberation in 1944. [1] Chronology. Tournai (before 486), current-day Belgium; Soissons (486-936) Laon (936-987) Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.\\n\\n[What was the capital of France before Paris? - StudyCountry.com](https://www.studycountry.com/wiki/what-was-the-capital-of-france-before-paris)\\nWho moved the capital of France to Paris? The Frankish king Clovis I had taken Paris from the Gauls by 494 ce and later made his capital there. Under Hugh Capet (ruled 987-996) and the Capetian dynasty the preeminence of Paris was firmly established, and Paris became the political and cultural hub as modern France took shape.\\n\\n[What happened to France's other capitals? - The French History Podcast](https://www.thefrenchhistorypodcast.com/what-happened-to-frances-other-capitals/)\\nThis was a desperate gamble meant to conquer France before enough American reinforcements could land in the country and turn the tide of war. ... In a supreme twist of irony, de Gaulle's organization claimed Algiers was the (temporary) capital of France after 100 years of Paris being the capital of colonial Algeria. On 6 June 1944 the Allies ...\\n\\n[Capital of France - Simple English Wikipedia, the free encyclopedia](https://simple.wikipedia.org/wiki/Capital_of_France)\\nThe capital of France is Paris. [1] In the course of history, the national capital has been in many locations other than Paris. History ... Tournai (before 486) Soissons (486-ca. 900) Paris (900-1419) The residence of the kings of France, but they were consecrated at Reims.\\n\\n[List of capitals of France - Wikiwand](https://www.wikiwand.com/en/articles/List_of_capitals_of_France)\\nTournai (before 486), current-day Belgium; Soissons (486-936); Laon (936-987); Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.; Orlans (1108), one of the few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, Archbishop of Sens; from ...\\n\\n[Where was the capital of France moved to?](https://www.ncesc.com/geographic-faq/where-was-the-capital-of-france-moved-to/)\\nThe first capital of France was the city of Paris. It has been the capital since the 10th century. Metz. Pepin the Short constructed a palace in Aachen, the exact year is not known and Aachen became the Seat of the King in the 8th century. Where was the French capital before Paris? However, Paris has not been the only capital in the country's ...\\n\\n[Why was Lutetia renamed Paris? - Geographic FAQ Hub: Answers to ... - NCESC](https://www.ncesc.com/geographic-faq/why-was-lutetia-renamed-paris/)\\nBefore Paris, the capital of France was located in various cities throughout history. However, one notable city that served as the capital before Paris was Vichy, which was the de facto capital of France during the French State from 1940 to 1944.\\n\\n[Capital of France facts for kids - Kids encyclopedia](https://kids.kiddle.co/Capital_of_France)\\nHistory List of capitals of France. Tournai (before 486); Soissons (486-ca. 900); Paris (900-1419) The residence of the kings of France, but they were consecrated at Reims.; Orlans (1108) One of the few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, archbishop of Sens and from 13 ...\\n\\n[Laon, medieval capital of France - Notes from Camelid Country](https://notesfromcamelidcountry.net/2017/08/18/laon-medieval-capital-of-france/)\\nIt's almost impossible to imagine today, but long before Paris became capital of France and went on to become one of the most famous cities on the planet, Laon, a little know and remarkably tourist-free town near the Belgian border, was the medieval capital of France. It gained this distinction during the reign of the Carolingian dynasty, the ...\\n\\n[What is the Capital of France? Paris - Countryaah.com](https://www.countryaah.com/france-faqs/)\\nOther Cities That Served as Capital in France's History 1. Soissons (Before 508 - 508) Soissons was the capital of the Merovingian Kingdom of the Franks before Paris took over the role. King Clovis I established Soissons as the capital after his victory at the Battle of Soissons. In 508, Clovis moved the capital to Paris, where it has ...\\nLast output from code snippet:\\nNone\", action_output=None),\n",
       " ActionStep(agent_memory=[{'role': <MessageRole.SYSTEM: 'system'>, 'content': 'You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.\\nTo do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.\\nTo solve the task, you must plan forward to proceed in a series of steps, in a cycle of \\'Thought:\\', \\'Code:\\', and \\'Observation:\\' sequences.\\n\\nAt each step, in the \\'Thought:\\' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.\\nThen in the \\'Code:\\' sequence, you should write the code in simple Python. The code sequence must end with \\'<end_code>\\' sequence.\\nDuring each intermediate step, you can use \\'print()\\' to save whatever important information you will then need.\\nThese print outputs will then appear in the \\'Observation:\\' field, which will be available as input for the next step.\\nIn the end you have to return a final answer using the `final_answer` tool.\\n\\nHere are a few examples using notional tools:\\n---\\nTask: \"Generate an image of the oldest person in this document.\"\\n\\nThought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\\nCode:\\n```py\\nanswer = document_qa(document=document, question=\"Who is the oldest person mentioned?\")\\nprint(answer)\\n```<end_code>\\nObservation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\\n\\nThought: I will now generate an image showcasing the oldest person.\\nCode:\\n```py\\nimage = image_generator(\"A portrait of John Doe, a 55-year-old man living in Canada.\")\\nfinal_answer(image)\\n```<end_code>\\n\\n---\\nTask: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\\n\\nThought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool\\nCode:\\n```py\\nresult = 5 + 3 + 1294.678\\nfinal_answer(result)\\n```<end_code>\\n\\n---\\nTask:\\n\"Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.\\nYou have been provided with these additional arguments, that you can access using the keys as variables in your python code:\\n{\\'question\\': \\'Quel est l\\'animal sur l\\'image?\\', \\'image\\': \\'path/to/image.jpg\\'}\"\\n\\nThought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.\\nCode:\\n```py\\ntranslated_question = translator(question=question, src_lang=\"French\", tgt_lang=\"English\")\\nprint(f\"The translated question is {translated_question}.\")\\nanswer = image_qa(image=image, question=translated_question)\\nfinal_answer(f\"The answer is {answer}\")\\n```<end_code>\\n\\n---\\nTask:\\nIn a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.\\nWhat does he say was the consequence of Einstein learning too much math on his creativity, in one word?\\n\\nThought: I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.\\nCode:\\n```py\\npages = search(query=\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\")\\nprint(pages)\\n```<end_code>\\nObservation:\\nNo result found for query \"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\".\\n\\nThought: The query was maybe too restrictive and did not find any results. Let\\'s try again with a broader query.\\nCode:\\n```py\\npages = search(query=\"1979 interview Stanislaus Ulam\")\\nprint(pages)\\n```<end_code>\\nObservation:\\nFound 6 pages:\\n[Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)\\n\\n[Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)\\n\\n(truncated)\\n\\nThought: I will read the first 2 pages to know more.\\nCode:\\n```py\\nfor url in [\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/\", \"https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/\"]:\\n    whole_page = visit_webpage(url)\\n    print(whole_page)\\n    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Print separator between pages\\n```<end_code>\\nObservation:\\nManhattan Project Locations:\\nLos Alamos, NM\\nStanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at\\n(truncated)\\n\\nThought: I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: \"He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity.\" Let\\'s answer in one word.\\nCode:\\n```py\\nfinal_answer(\"diminished\")\\n```<end_code>\\n\\n---\\nTask: \"Which city has the highest population: Guangzhou or Shanghai?\"\\n\\nThought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\\nCode:\\n```py\\nfor city in [\"Guangzhou\", \"Shanghai\"]:\\n    print(f\"Population {city}:\", search(f\"{city} population\")\\n```<end_code>\\nObservation:\\nPopulation Guangzhou: [\\'Guangzhou has a population of 15 million inhabitants as of 2021.\\']\\nPopulation Shanghai: \\'26 million (2019)\\'\\n\\nThought: Now I know that Shanghai has the highest population.\\nCode:\\n```py\\nfinal_answer(\"Shanghai\")\\n```<end_code>\\n\\n---\\nTask: \"What is the current age of the pope, raised to the power 0.36?\"\\n\\nThought: I will use the tool `wiki` to get the age of the pope, and confirm that with a web search.\\nCode:\\n```py\\npope_age_wiki = wiki(query=\"current pope age\")\\nprint(\"Pope age as per wikipedia:\", pope_age_wiki)\\npope_age_search = web_search(query=\"current pope age\")\\nprint(\"Pope age as per google search:\", pope_age_search)\\n```<end_code>\\nObservation:\\nPope age: \"The pope Francis is currently 88 years old.\"\\n\\nThought: I know that the pope is 88 years old. Let\\'s compute the result using python code.\\nCode:\\n```py\\npope_current_age = 88 ** 0.36\\nfinal_answer(pope_current_age)\\n```<end_code>\\n\\nAbove example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools:\\n\\n\\n- web_search: Performs a duckduckgo web search based on your query (think a Google search) then returns the top search results.\\n    Takes inputs: {\\'query\\': {\\'type\\': \\'string\\', \\'description\\': \\'The search query to perform.\\'}}\\n    Returns an output of type: string\\n\\n- final_answer: Provides a final answer to the given problem.\\n    Takes inputs: {\\'answer\\': {\\'type\\': \\'any\\', \\'description\\': \\'The final answer to the problem\\'}}\\n    Returns an output of type: any\\n\\n\\n\\nHere are the rules you should always follow to solve your task:\\n1. Always provide a \\'Thought:\\' sequence, and a \\'Code:\\n```py\\' sequence ending with \\'```<end_code>\\' sequence, else you will fail.\\n2. Use only variables that you have defined!\\n3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in \\'answer = wiki({\\'query\\': \"What is the place where James Bond lives?\"})\\', but use the arguments directly as in \\'answer = wiki(query=\"What is the place where James Bond lives?\")\\'.\\n4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.\\n5. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.\\n6. Don\\'t name any new variable with the same name as a tool: for instance don\\'t name a variable \\'final_answer\\'.\\n7. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.\\n8. You can use imports in your code, but only from the following list of modules: {{authorized_imports}}\\n9. The state persists between code executions: so if in one step you\\'ve created variables or imported modules, these will all persist.\\n10. Don\\'t give up! You\\'re in charge of solving the task, not providing directions to solve it.\\n\\nNow Begin! If you solve the task correctly, you will receive a reward of $1,000,000.'}, {'role': <MessageRole.USER: 'user'>, 'content': 'New task:\\nWhat was the capital of France before Paris?'}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': 'Thought: To determine the capital of France before Paris, I\\'ll perform a web search for historical information about the capital of France.\\n\\nCode:\\n```py\\nresult = web_search(query=\"capital of France before Paris\")\\nprint(result)\\n```<end_code>'}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': '[{\\'id\\': \\'call_2\\', \\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'python_interpreter\\', \\'arguments\\': \\'result = web_search(query=\"capital of France before Paris\")\\\\nprint(result)\\'}}]'}, {'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': \"Call id: call_2\\nObservation:\\nExecution logs:\\n## Search Results\\n\\n[List of capitals of France - Wikipedia](https://en.wikipedia.org/wiki/List_of_capitals_of_France)\\nThis is a chronological list of capitals of France. The capital of France has been Paris since its liberation in 1944. [1] Chronology. Tournai (before 486), current-day Belgium; Soissons (486-936) Laon (936-987) Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.\\n\\n[What was the capital of France before Paris? - StudyCountry.com](https://www.studycountry.com/wiki/what-was-the-capital-of-france-before-paris)\\nWho moved the capital of France to Paris? The Frankish king Clovis I had taken Paris from the Gauls by 494 ce and later made his capital there. Under Hugh Capet (ruled 987-996) and the Capetian dynasty the preeminence of Paris was firmly established, and Paris became the political and cultural hub as modern France took shape.\\n\\n[What happened to France's other capitals? - The French History Podcast](https://www.thefrenchhistorypodcast.com/what-happened-to-frances-other-capitals/)\\nThis was a desperate gamble meant to conquer France before enough American reinforcements could land in the country and turn the tide of war. ... In a supreme twist of irony, de Gaulle's organization claimed Algiers was the (temporary) capital of France after 100 years of Paris being the capital of colonial Algeria. On 6 June 1944 the Allies ...\\n\\n[Capital of France - Simple English Wikipedia, the free encyclopedia](https://simple.wikipedia.org/wiki/Capital_of_France)\\nThe capital of France is Paris. [1] In the course of history, the national capital has been in many locations other than Paris. History ... Tournai (before 486) Soissons (486-ca. 900) Paris (900-1419) The residence of the kings of France, but they were consecrated at Reims.\\n\\n[List of capitals of France - Wikiwand](https://www.wikiwand.com/en/articles/List_of_capitals_of_France)\\nTournai (before 486), current-day Belgium; Soissons (486-936); Laon (936-987); Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.; Orlans (1108), one of the few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, Archbishop of Sens; from ...\\n\\n[Where was the capital of France moved to?](https://www.ncesc.com/geographic-faq/where-was-the-capital-of-france-moved-to/)\\nThe first capital of France was the city of Paris. It has been the capital since the 10th century. Metz. Pepin the Short constructed a palace in Aachen, the exact year is not known and Aachen became the Seat of the King in the 8th century. Where was the French capital before Paris? However, Paris has not been the only capital in the country's ...\\n\\n[Why was Lutetia renamed Paris? - Geographic FAQ Hub: Answers to ... - NCESC](https://www.ncesc.com/geographic-faq/why-was-lutetia-renamed-paris/)\\nBefore Paris, the capital of France was located in various cities throughout history. However, one notable city that served as the capital before Paris was Vichy, which was the de facto capital of France during the French State from 1940 to 1944.\\n\\n[Capital of France facts for kids - Kids encyclopedia](https://kids.kiddle.co/Capital_of_France)\\nHistory List of capitals of France. Tournai (before 486); Soissons (486-ca. 900); Paris (900-1419) The residence of the kings of France, but they were consecrated at Reims.; Orlans (1108) One of the few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, archbishop of Sens and from 13 ...\\n\\n[Laon, medieval capital of France - Notes from Camelid Country](https://notesfromcamelidcountry.net/2017/08/18/laon-medieval-capital-of-france/)\\nIt's almost impossible to imagine today, but long before Paris became capital of France and went on to become one of the most famous cities on the planet, Laon, a little know and remarkably tourist-free town near the Belgian border, was the medieval capital of France. It gained this distinction during the reign of the Carolingian dynasty, the ...\\n\\n[What is the Capital of France? Paris - Countryaah.com](https://www.countryaah.com/france-faqs/)\\nOther Cities That Served as Capital in France's History 1. Soissons (Before 508 - 508) Soissons was the capital of the Merovingian Kingdom of the Franks before Paris took over the role. King Clovis I established Soissons as the capital after his victory at the Battle of Soissons. In 508, Clovis moved the capital to Paris, where it has ...\\nLast output from code snippet:\\nNone\"}], tool_calls=[ToolCall(name='python_interpreter', arguments='result = web_search(query=\"Laon capital of France before Paris\")\\nprint(result)', id='call_3')], start_time=1737104845.9660962, end_time=1737104850.0077112, step=1, error=AgentExecutionError('https://lite.duckduckgo.com/lite/ 202 Ratelimit'), duration=4.041615009307861, llm_output='Thought: Based on the search results, it appears that the capital of France before Paris was Soissons. To confirm this, I\\'ll check if there\\'s any mention of Laon being the capital before Paris, as it was mentioned in some of the search results.\\n\\nCode:\\n```py\\nresult = web_search(query=\"Laon capital of France before Paris\")\\nprint(result)\\n```<end_code>', observations=None, action_output=None),\n",
       " ActionStep(agent_memory=[{'role': <MessageRole.SYSTEM: 'system'>, 'content': 'You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.\\nTo do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.\\nTo solve the task, you must plan forward to proceed in a series of steps, in a cycle of \\'Thought:\\', \\'Code:\\', and \\'Observation:\\' sequences.\\n\\nAt each step, in the \\'Thought:\\' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.\\nThen in the \\'Code:\\' sequence, you should write the code in simple Python. The code sequence must end with \\'<end_code>\\' sequence.\\nDuring each intermediate step, you can use \\'print()\\' to save whatever important information you will then need.\\nThese print outputs will then appear in the \\'Observation:\\' field, which will be available as input for the next step.\\nIn the end you have to return a final answer using the `final_answer` tool.\\n\\nHere are a few examples using notional tools:\\n---\\nTask: \"Generate an image of the oldest person in this document.\"\\n\\nThought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\\nCode:\\n```py\\nanswer = document_qa(document=document, question=\"Who is the oldest person mentioned?\")\\nprint(answer)\\n```<end_code>\\nObservation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\\n\\nThought: I will now generate an image showcasing the oldest person.\\nCode:\\n```py\\nimage = image_generator(\"A portrait of John Doe, a 55-year-old man living in Canada.\")\\nfinal_answer(image)\\n```<end_code>\\n\\n---\\nTask: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\\n\\nThought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool\\nCode:\\n```py\\nresult = 5 + 3 + 1294.678\\nfinal_answer(result)\\n```<end_code>\\n\\n---\\nTask:\\n\"Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.\\nYou have been provided with these additional arguments, that you can access using the keys as variables in your python code:\\n{\\'question\\': \\'Quel est l\\'animal sur l\\'image?\\', \\'image\\': \\'path/to/image.jpg\\'}\"\\n\\nThought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.\\nCode:\\n```py\\ntranslated_question = translator(question=question, src_lang=\"French\", tgt_lang=\"English\")\\nprint(f\"The translated question is {translated_question}.\")\\nanswer = image_qa(image=image, question=translated_question)\\nfinal_answer(f\"The answer is {answer}\")\\n```<end_code>\\n\\n---\\nTask:\\nIn a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.\\nWhat does he say was the consequence of Einstein learning too much math on his creativity, in one word?\\n\\nThought: I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.\\nCode:\\n```py\\npages = search(query=\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\")\\nprint(pages)\\n```<end_code>\\nObservation:\\nNo result found for query \"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\".\\n\\nThought: The query was maybe too restrictive and did not find any results. Let\\'s try again with a broader query.\\nCode:\\n```py\\npages = search(query=\"1979 interview Stanislaus Ulam\")\\nprint(pages)\\n```<end_code>\\nObservation:\\nFound 6 pages:\\n[Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)\\n\\n[Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)\\n\\n(truncated)\\n\\nThought: I will read the first 2 pages to know more.\\nCode:\\n```py\\nfor url in [\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/\", \"https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/\"]:\\n    whole_page = visit_webpage(url)\\n    print(whole_page)\\n    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Print separator between pages\\n```<end_code>\\nObservation:\\nManhattan Project Locations:\\nLos Alamos, NM\\nStanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at\\n(truncated)\\n\\nThought: I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: \"He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity.\" Let\\'s answer in one word.\\nCode:\\n```py\\nfinal_answer(\"diminished\")\\n```<end_code>\\n\\n---\\nTask: \"Which city has the highest population: Guangzhou or Shanghai?\"\\n\\nThought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\\nCode:\\n```py\\nfor city in [\"Guangzhou\", \"Shanghai\"]:\\n    print(f\"Population {city}:\", search(f\"{city} population\")\\n```<end_code>\\nObservation:\\nPopulation Guangzhou: [\\'Guangzhou has a population of 15 million inhabitants as of 2021.\\']\\nPopulation Shanghai: \\'26 million (2019)\\'\\n\\nThought: Now I know that Shanghai has the highest population.\\nCode:\\n```py\\nfinal_answer(\"Shanghai\")\\n```<end_code>\\n\\n---\\nTask: \"What is the current age of the pope, raised to the power 0.36?\"\\n\\nThought: I will use the tool `wiki` to get the age of the pope, and confirm that with a web search.\\nCode:\\n```py\\npope_age_wiki = wiki(query=\"current pope age\")\\nprint(\"Pope age as per wikipedia:\", pope_age_wiki)\\npope_age_search = web_search(query=\"current pope age\")\\nprint(\"Pope age as per google search:\", pope_age_search)\\n```<end_code>\\nObservation:\\nPope age: \"The pope Francis is currently 88 years old.\"\\n\\nThought: I know that the pope is 88 years old. Let\\'s compute the result using python code.\\nCode:\\n```py\\npope_current_age = 88 ** 0.36\\nfinal_answer(pope_current_age)\\n```<end_code>\\n\\nAbove example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools:\\n\\n\\n- web_search: Performs a duckduckgo web search based on your query (think a Google search) then returns the top search results.\\n    Takes inputs: {\\'query\\': {\\'type\\': \\'string\\', \\'description\\': \\'The search query to perform.\\'}}\\n    Returns an output of type: string\\n\\n- final_answer: Provides a final answer to the given problem.\\n    Takes inputs: {\\'answer\\': {\\'type\\': \\'any\\', \\'description\\': \\'The final answer to the problem\\'}}\\n    Returns an output of type: any\\n\\n\\n\\nHere are the rules you should always follow to solve your task:\\n1. Always provide a \\'Thought:\\' sequence, and a \\'Code:\\n```py\\' sequence ending with \\'```<end_code>\\' sequence, else you will fail.\\n2. Use only variables that you have defined!\\n3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in \\'answer = wiki({\\'query\\': \"What is the place where James Bond lives?\"})\\', but use the arguments directly as in \\'answer = wiki(query=\"What is the place where James Bond lives?\")\\'.\\n4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.\\n5. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.\\n6. Don\\'t name any new variable with the same name as a tool: for instance don\\'t name a variable \\'final_answer\\'.\\n7. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.\\n8. You can use imports in your code, but only from the following list of modules: {{authorized_imports}}\\n9. The state persists between code executions: so if in one step you\\'ve created variables or imported modules, these will all persist.\\n10. Don\\'t give up! You\\'re in charge of solving the task, not providing directions to solve it.\\n\\nNow Begin! If you solve the task correctly, you will receive a reward of $1,000,000.'}, {'role': <MessageRole.USER: 'user'>, 'content': 'New task:\\nWhat was the capital of France before Paris?'}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': 'Thought: To determine the capital of France before Paris, I\\'ll perform a web search for historical information about the capital of France.\\n\\nCode:\\n```py\\nresult = web_search(query=\"capital of France before Paris\")\\nprint(result)\\n```<end_code>'}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': '[{\\'id\\': \\'call_2\\', \\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'python_interpreter\\', \\'arguments\\': \\'result = web_search(query=\"capital of France before Paris\")\\\\nprint(result)\\'}}]'}, {'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': \"Call id: call_2\\nObservation:\\nExecution logs:\\n## Search Results\\n\\n[List of capitals of France - Wikipedia](https://en.wikipedia.org/wiki/List_of_capitals_of_France)\\nThis is a chronological list of capitals of France. The capital of France has been Paris since its liberation in 1944. [1] Chronology. Tournai (before 486), current-day Belgium; Soissons (486-936) Laon (936-987) Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.\\n\\n[What was the capital of France before Paris? - StudyCountry.com](https://www.studycountry.com/wiki/what-was-the-capital-of-france-before-paris)\\nWho moved the capital of France to Paris? The Frankish king Clovis I had taken Paris from the Gauls by 494 ce and later made his capital there. Under Hugh Capet (ruled 987-996) and the Capetian dynasty the preeminence of Paris was firmly established, and Paris became the political and cultural hub as modern France took shape.\\n\\n[What happened to France's other capitals? - The French History Podcast](https://www.thefrenchhistorypodcast.com/what-happened-to-frances-other-capitals/)\\nThis was a desperate gamble meant to conquer France before enough American reinforcements could land in the country and turn the tide of war. ... In a supreme twist of irony, de Gaulle's organization claimed Algiers was the (temporary) capital of France after 100 years of Paris being the capital of colonial Algeria. On 6 June 1944 the Allies ...\\n\\n[Capital of France - Simple English Wikipedia, the free encyclopedia](https://simple.wikipedia.org/wiki/Capital_of_France)\\nThe capital of France is Paris. [1] In the course of history, the national capital has been in many locations other than Paris. History ... Tournai (before 486) Soissons (486-ca. 900) Paris (900-1419) The residence of the kings of France, but they were consecrated at Reims.\\n\\n[List of capitals of France - Wikiwand](https://www.wikiwand.com/en/articles/List_of_capitals_of_France)\\nTournai (before 486), current-day Belgium; Soissons (486-936); Laon (936-987); Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.; Orlans (1108), one of the few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, Archbishop of Sens; from ...\\n\\n[Where was the capital of France moved to?](https://www.ncesc.com/geographic-faq/where-was-the-capital-of-france-moved-to/)\\nThe first capital of France was the city of Paris. It has been the capital since the 10th century. Metz. Pepin the Short constructed a palace in Aachen, the exact year is not known and Aachen became the Seat of the King in the 8th century. Where was the French capital before Paris? However, Paris has not been the only capital in the country's ...\\n\\n[Why was Lutetia renamed Paris? - Geographic FAQ Hub: Answers to ... - NCESC](https://www.ncesc.com/geographic-faq/why-was-lutetia-renamed-paris/)\\nBefore Paris, the capital of France was located in various cities throughout history. However, one notable city that served as the capital before Paris was Vichy, which was the de facto capital of France during the French State from 1940 to 1944.\\n\\n[Capital of France facts for kids - Kids encyclopedia](https://kids.kiddle.co/Capital_of_France)\\nHistory List of capitals of France. Tournai (before 486); Soissons (486-ca. 900); Paris (900-1419) The residence of the kings of France, but they were consecrated at Reims.; Orlans (1108) One of the few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, archbishop of Sens and from 13 ...\\n\\n[Laon, medieval capital of France - Notes from Camelid Country](https://notesfromcamelidcountry.net/2017/08/18/laon-medieval-capital-of-france/)\\nIt's almost impossible to imagine today, but long before Paris became capital of France and went on to become one of the most famous cities on the planet, Laon, a little know and remarkably tourist-free town near the Belgian border, was the medieval capital of France. It gained this distinction during the reign of the Carolingian dynasty, the ...\\n\\n[What is the Capital of France? Paris - Countryaah.com](https://www.countryaah.com/france-faqs/)\\nOther Cities That Served as Capital in France's History 1. Soissons (Before 508 - 508) Soissons was the capital of the Merovingian Kingdom of the Franks before Paris took over the role. King Clovis I established Soissons as the capital after his victory at the Battle of Soissons. In 508, Clovis moved the capital to Paris, where it has ...\\nLast output from code snippet:\\nNone\"}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': 'Thought: Based on the search results, it appears that the capital of France before Paris was Soissons. To confirm this, I\\'ll check if there\\'s any mention of Laon being the capital before Paris, as it was mentioned in some of the search results.\\n\\nCode:\\n```py\\nresult = web_search(query=\"Laon capital of France before Paris\")\\nprint(result)\\n```<end_code>'}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': '[{\\'id\\': \\'call_3\\', \\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'python_interpreter\\', \\'arguments\\': \\'result = web_search(query=\"Laon capital of France before Paris\")\\\\nprint(result)\\'}}]'}, {'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': \"Call id: call_3\\nError:\\nhttps://lite.duckduckgo.com/lite/ 202 Ratelimit\\nNow let's retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\\n\"}], tool_calls=[ToolCall(name='python_interpreter', arguments='result = web_search(query=\"capital of France before 987 AD\")\\nprint(result)', id='call_4')], start_time=1737104850.0081801, end_time=1737104853.6308122, step=2, error=None, duration=3.6226320266723633, llm_output='Thought: Given the ratelimit error, I\\'ll try a different approach. I\\'ll search for a more specific query to confirm the capital before Paris, focusing on the period before the 10th century.\\n\\nCode:\\n```py\\nresult = web_search(query=\"capital of France before 987 AD\")\\nprint(result)\\n```<end_code>\\n', observations='Execution logs:\\n## Search Results\\n\\n[List of capitals of France - Wikipedia](https://en.wikipedia.org/wiki/List_of_capitals_of_France)\\nThe capital of France has been Paris since its liberation in 1944. [1] Chronology. Tournai (before 486), current-day Belgium; Soissons (486-936) Laon (936-987) Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.\\n\\n[Laon, medieval capital of France - Notes from Camelid Country](https://notesfromcamelidcountry.net/2017/08/18/laon-medieval-capital-of-france/)\\nIt\\'s almost impossible to imagine today, but long before Paris became capital of France and went on to become one of the most famous cities on the planet, Laon, a little know and remarkably tourist-free town near the Belgian border, was the medieval capital of France. ... Power shifted to Paris in 987 AD when Hugues Capet claimed the crown ...\\n\\n[France in the Middle Ages - Wikipedia](https://en.wikipedia.org/wiki/France_in_the_Middle_Ages)\\nAt the end of the Middle Ages, France was the most populous region [clarification needed] in Europehaving overtaken Spain and Italy by 1340. [2] In the 14th century, before the arrival of the Black Death, the total population of the area covered by modern-day France has been estimated at 16 million. [3] The population of Paris is controversial. [4] ...\\n\\n[What was the capital of France before Paris? - StudyCountry.com](https://www.studycountry.com/wiki/what-was-the-capital-of-france-before-paris)\\nWhat was the original capital of France? Christianity arrived in the 2nd century AD, and the 5th century saw the end of Roman rule with the arrival of the Franks. In 508 AD, Clovis I, the Frankish king, united Gaul as a kingdom and established Paris as the capital, naming it in honor of the original Parisii tribe.\\n\\n[Kingdom of France - Wikipedia](https://en.wikipedia.org/wiki/Kingdom_of_France)\\nThe Kingdom of France is the historiographical name or umbrella term given to various political entities of France in the medieval and early modern period. It was one of the most powerful states in Europe from the High Middle Ages to 1848 during its dissolution. It was also an early colonial power, with colonies in Asia and Africa, and the largest being New France in North America centred ...\\n\\n[What happened to France\\'s other capitals? - The French History Podcast](https://www.thefrenchhistorypodcast.com/what-happened-to-frances-other-capitals/)\\nHowever, in France\\'s long history other cities have taken that role. In the modern period it\\'s very clear which cities are the capital as they hold the permanent seat of government. Before that, the capital was usually wherever the king most commonly held court. When Hugh Capet became king in 987 he ruled France from his powerbase in the north.\\n\\n[How did France get its name? - Geographic Pedia](https://www.ncesc.com/geographic-pedia/how-did-france-get-its-name/)\\nFrance Before its Name. ... Gaul was also referred to as Frankia when it was first conquered by the Franks in the 5th century AD. The Franks, a Germanic tribe, bestowed their name upon the region, eventually leading to the name \"France.\" ... In 987 A.D., Paris officially became the capital of France, solidifying its place in history. French ...\\n\\n[When did paris first become the capital of france?](https://www.reddit.com/r/AskHistorians/comments/18yz331/when_did_paris_first_become_the_capital_of_france/)\\nWhen did paris first become the capital of france? Okay having some articals online saying it was in 987 but i thought it was the capital during Charles the balds reign am I misremembering or is the internet oversimplifying things? ... Please read the rules before participating, as we remove all comments which break the rules. ...\\n\\n[Kingdom of West Francia Timeline - World History Encyclopedia](https://www.worldhistory.org/timeline/Kingdom_of_West_Francia/)\\nThe Kingdom of West Francia (843-987 CE, also known as The Kingdom of the West Franks) was the region of Western Europe that formed the western part of the Carolingian Empire of Charlemagne (Holy Roman Emperor 800-814 CE) known as Francia or the Kingdom of the Franks. ... Birth of Charles the Simple\\'s and Eadgifu\\'s son Louis (later Louis IV of ...\\n\\n[The birth of the french capital - History of Paris city](http://www.paris-city.fr/GB/paris-city/au-fil-du-temps/histoire.php)\\nBIRTH OF A CAPITAL Discover the history of Paris through the centuries Paris was founded in the 3rd century B.C. on le de la Cit by a community of Celts. They were a group of tribal fishermen called the Parisii who, pushed by emigration towards the banks of the Seine, made a permanent settlement there and profited from the area\\'s fertility and temperate climate.\\nLast output from code snippet:\\nNone', action_output=None),\n",
       " ActionStep(agent_memory=[{'role': <MessageRole.SYSTEM: 'system'>, 'content': 'You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.\\nTo do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.\\nTo solve the task, you must plan forward to proceed in a series of steps, in a cycle of \\'Thought:\\', \\'Code:\\', and \\'Observation:\\' sequences.\\n\\nAt each step, in the \\'Thought:\\' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.\\nThen in the \\'Code:\\' sequence, you should write the code in simple Python. The code sequence must end with \\'<end_code>\\' sequence.\\nDuring each intermediate step, you can use \\'print()\\' to save whatever important information you will then need.\\nThese print outputs will then appear in the \\'Observation:\\' field, which will be available as input for the next step.\\nIn the end you have to return a final answer using the `final_answer` tool.\\n\\nHere are a few examples using notional tools:\\n---\\nTask: \"Generate an image of the oldest person in this document.\"\\n\\nThought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\\nCode:\\n```py\\nanswer = document_qa(document=document, question=\"Who is the oldest person mentioned?\")\\nprint(answer)\\n```<end_code>\\nObservation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\\n\\nThought: I will now generate an image showcasing the oldest person.\\nCode:\\n```py\\nimage = image_generator(\"A portrait of John Doe, a 55-year-old man living in Canada.\")\\nfinal_answer(image)\\n```<end_code>\\n\\n---\\nTask: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\\n\\nThought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool\\nCode:\\n```py\\nresult = 5 + 3 + 1294.678\\nfinal_answer(result)\\n```<end_code>\\n\\n---\\nTask:\\n\"Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.\\nYou have been provided with these additional arguments, that you can access using the keys as variables in your python code:\\n{\\'question\\': \\'Quel est l\\'animal sur l\\'image?\\', \\'image\\': \\'path/to/image.jpg\\'}\"\\n\\nThought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.\\nCode:\\n```py\\ntranslated_question = translator(question=question, src_lang=\"French\", tgt_lang=\"English\")\\nprint(f\"The translated question is {translated_question}.\")\\nanswer = image_qa(image=image, question=translated_question)\\nfinal_answer(f\"The answer is {answer}\")\\n```<end_code>\\n\\n---\\nTask:\\nIn a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.\\nWhat does he say was the consequence of Einstein learning too much math on his creativity, in one word?\\n\\nThought: I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.\\nCode:\\n```py\\npages = search(query=\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\")\\nprint(pages)\\n```<end_code>\\nObservation:\\nNo result found for query \"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\".\\n\\nThought: The query was maybe too restrictive and did not find any results. Let\\'s try again with a broader query.\\nCode:\\n```py\\npages = search(query=\"1979 interview Stanislaus Ulam\")\\nprint(pages)\\n```<end_code>\\nObservation:\\nFound 6 pages:\\n[Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)\\n\\n[Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)\\n\\n(truncated)\\n\\nThought: I will read the first 2 pages to know more.\\nCode:\\n```py\\nfor url in [\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/\", \"https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/\"]:\\n    whole_page = visit_webpage(url)\\n    print(whole_page)\\n    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Print separator between pages\\n```<end_code>\\nObservation:\\nManhattan Project Locations:\\nLos Alamos, NM\\nStanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at\\n(truncated)\\n\\nThought: I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: \"He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity.\" Let\\'s answer in one word.\\nCode:\\n```py\\nfinal_answer(\"diminished\")\\n```<end_code>\\n\\n---\\nTask: \"Which city has the highest population: Guangzhou or Shanghai?\"\\n\\nThought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\\nCode:\\n```py\\nfor city in [\"Guangzhou\", \"Shanghai\"]:\\n    print(f\"Population {city}:\", search(f\"{city} population\")\\n```<end_code>\\nObservation:\\nPopulation Guangzhou: [\\'Guangzhou has a population of 15 million inhabitants as of 2021.\\']\\nPopulation Shanghai: \\'26 million (2019)\\'\\n\\nThought: Now I know that Shanghai has the highest population.\\nCode:\\n```py\\nfinal_answer(\"Shanghai\")\\n```<end_code>\\n\\n---\\nTask: \"What is the current age of the pope, raised to the power 0.36?\"\\n\\nThought: I will use the tool `wiki` to get the age of the pope, and confirm that with a web search.\\nCode:\\n```py\\npope_age_wiki = wiki(query=\"current pope age\")\\nprint(\"Pope age as per wikipedia:\", pope_age_wiki)\\npope_age_search = web_search(query=\"current pope age\")\\nprint(\"Pope age as per google search:\", pope_age_search)\\n```<end_code>\\nObservation:\\nPope age: \"The pope Francis is currently 88 years old.\"\\n\\nThought: I know that the pope is 88 years old. Let\\'s compute the result using python code.\\nCode:\\n```py\\npope_current_age = 88 ** 0.36\\nfinal_answer(pope_current_age)\\n```<end_code>\\n\\nAbove example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools:\\n\\n\\n- web_search: Performs a duckduckgo web search based on your query (think a Google search) then returns the top search results.\\n    Takes inputs: {\\'query\\': {\\'type\\': \\'string\\', \\'description\\': \\'The search query to perform.\\'}}\\n    Returns an output of type: string\\n\\n- final_answer: Provides a final answer to the given problem.\\n    Takes inputs: {\\'answer\\': {\\'type\\': \\'any\\', \\'description\\': \\'The final answer to the problem\\'}}\\n    Returns an output of type: any\\n\\n\\n\\nHere are the rules you should always follow to solve your task:\\n1. Always provide a \\'Thought:\\' sequence, and a \\'Code:\\n```py\\' sequence ending with \\'```<end_code>\\' sequence, else you will fail.\\n2. Use only variables that you have defined!\\n3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in \\'answer = wiki({\\'query\\': \"What is the place where James Bond lives?\"})\\', but use the arguments directly as in \\'answer = wiki(query=\"What is the place where James Bond lives?\")\\'.\\n4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.\\n5. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.\\n6. Don\\'t name any new variable with the same name as a tool: for instance don\\'t name a variable \\'final_answer\\'.\\n7. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.\\n8. You can use imports in your code, but only from the following list of modules: {{authorized_imports}}\\n9. The state persists between code executions: so if in one step you\\'ve created variables or imported modules, these will all persist.\\n10. Don\\'t give up! You\\'re in charge of solving the task, not providing directions to solve it.\\n\\nNow Begin! If you solve the task correctly, you will receive a reward of $1,000,000.'}, {'role': <MessageRole.USER: 'user'>, 'content': 'New task:\\nWhat was the capital of France before Paris?'}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': 'Thought: To determine the capital of France before Paris, I\\'ll perform a web search for historical information about the capital of France.\\n\\nCode:\\n```py\\nresult = web_search(query=\"capital of France before Paris\")\\nprint(result)\\n```<end_code>'}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': '[{\\'id\\': \\'call_2\\', \\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'python_interpreter\\', \\'arguments\\': \\'result = web_search(query=\"capital of France before Paris\")\\\\nprint(result)\\'}}]'}, {'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': \"Call id: call_2\\nObservation:\\nExecution logs:\\n## Search Results\\n\\n[List of capitals of France - Wikipedia](https://en.wikipedia.org/wiki/List_of_capitals_of_France)\\nThis is a chronological list of capitals of France. The capital of France has been Paris since its liberation in 1944. [1] Chronology. Tournai (before 486), current-day Belgium; Soissons (486-936) Laon (936-987) Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.\\n\\n[What was the capital of France before Paris? - StudyCountry.com](https://www.studycountry.com/wiki/what-was-the-capital-of-france-before-paris)\\nWho moved the capital of France to Paris? The Frankish king Clovis I had taken Paris from the Gauls by 494 ce and later made his capital there. Under Hugh Capet (ruled 987-996) and the Capetian dynasty the preeminence of Paris was firmly established, and Paris became the political and cultural hub as modern France took shape.\\n\\n[What happened to France's other capitals? - The French History Podcast](https://www.thefrenchhistorypodcast.com/what-happened-to-frances-other-capitals/)\\nThis was a desperate gamble meant to conquer France before enough American reinforcements could land in the country and turn the tide of war. ... In a supreme twist of irony, de Gaulle's organization claimed Algiers was the (temporary) capital of France after 100 years of Paris being the capital of colonial Algeria. On 6 June 1944 the Allies ...\\n\\n[Capital of France - Simple English Wikipedia, the free encyclopedia](https://simple.wikipedia.org/wiki/Capital_of_France)\\nThe capital of France is Paris. [1] In the course of history, the national capital has been in many locations other than Paris. History ... Tournai (before 486) Soissons (486-ca. 900) Paris (900-1419) The residence of the kings of France, but they were consecrated at Reims.\\n\\n[List of capitals of France - Wikiwand](https://www.wikiwand.com/en/articles/List_of_capitals_of_France)\\nTournai (before 486), current-day Belgium; Soissons (486-936); Laon (936-987); Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.; Orlans (1108), one of the few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, Archbishop of Sens; from ...\\n\\n[Where was the capital of France moved to?](https://www.ncesc.com/geographic-faq/where-was-the-capital-of-france-moved-to/)\\nThe first capital of France was the city of Paris. It has been the capital since the 10th century. Metz. Pepin the Short constructed a palace in Aachen, the exact year is not known and Aachen became the Seat of the King in the 8th century. Where was the French capital before Paris? However, Paris has not been the only capital in the country's ...\\n\\n[Why was Lutetia renamed Paris? - Geographic FAQ Hub: Answers to ... - NCESC](https://www.ncesc.com/geographic-faq/why-was-lutetia-renamed-paris/)\\nBefore Paris, the capital of France was located in various cities throughout history. However, one notable city that served as the capital before Paris was Vichy, which was the de facto capital of France during the French State from 1940 to 1944.\\n\\n[Capital of France facts for kids - Kids encyclopedia](https://kids.kiddle.co/Capital_of_France)\\nHistory List of capitals of France. Tournai (before 486); Soissons (486-ca. 900); Paris (900-1419) The residence of the kings of France, but they were consecrated at Reims.; Orlans (1108) One of the few consecrations of a French monarch to occur outside of Reims occurred at Orlans, when Louis VI the Fat was consecrated in Orlans Cathedral by Daimbert, archbishop of Sens and from 13 ...\\n\\n[Laon, medieval capital of France - Notes from Camelid Country](https://notesfromcamelidcountry.net/2017/08/18/laon-medieval-capital-of-france/)\\nIt's almost impossible to imagine today, but long before Paris became capital of France and went on to become one of the most famous cities on the planet, Laon, a little know and remarkably tourist-free town near the Belgian border, was the medieval capital of France. It gained this distinction during the reign of the Carolingian dynasty, the ...\\n\\n[What is the Capital of France? Paris - Countryaah.com](https://www.countryaah.com/france-faqs/)\\nOther Cities That Served as Capital in France's History 1. Soissons (Before 508 - 508) Soissons was the capital of the Merovingian Kingdom of the Franks before Paris took over the role. King Clovis I established Soissons as the capital after his victory at the Battle of Soissons. In 508, Clovis moved the capital to Paris, where it has ...\\nLast output from code snippet:\\nNone\"}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': 'Thought: Based on the search results, it appears that the capital of France before Paris was Soissons. To confirm this, I\\'ll check if there\\'s any mention of Laon being the capital before Paris, as it was mentioned in some of the search results.\\n\\nCode:\\n```py\\nresult = web_search(query=\"Laon capital of France before Paris\")\\nprint(result)\\n```<end_code>'}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': '[{\\'id\\': \\'call_3\\', \\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'python_interpreter\\', \\'arguments\\': \\'result = web_search(query=\"Laon capital of France before Paris\")\\\\nprint(result)\\'}}]'}, {'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': \"Call id: call_3\\nError:\\nhttps://lite.duckduckgo.com/lite/ 202 Ratelimit\\nNow let's retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\\n\"}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': 'Thought: Given the ratelimit error, I\\'ll try a different approach. I\\'ll search for a more specific query to confirm the capital before Paris, focusing on the period before the 10th century.\\n\\nCode:\\n```py\\nresult = web_search(query=\"capital of France before 987 AD\")\\nprint(result)\\n```<end_code>'}, {'role': <MessageRole.ASSISTANT: 'assistant'>, 'content': '[{\\'id\\': \\'call_4\\', \\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'python_interpreter\\', \\'arguments\\': \\'result = web_search(query=\"capital of France before 987 AD\")\\\\nprint(result)\\'}}]'}, {'role': <MessageRole.TOOL_RESPONSE: 'tool-response'>, 'content': 'Call id: call_4\\nObservation:\\nExecution logs:\\n## Search Results\\n\\n[List of capitals of France - Wikipedia](https://en.wikipedia.org/wiki/List_of_capitals_of_France)\\nThe capital of France has been Paris since its liberation in 1944. [1] Chronology. Tournai (before 486), current-day Belgium; Soissons (486-936) Laon (936-987) Paris (987-1419), the residence of the Kings of France, although they were consecrated at Reims.\\n\\n[Laon, medieval capital of France - Notes from Camelid Country](https://notesfromcamelidcountry.net/2017/08/18/laon-medieval-capital-of-france/)\\nIt\\'s almost impossible to imagine today, but long before Paris became capital of France and went on to become one of the most famous cities on the planet, Laon, a little know and remarkably tourist-free town near the Belgian border, was the medieval capital of France. ... Power shifted to Paris in 987 AD when Hugues Capet claimed the crown ...\\n\\n[France in the Middle Ages - Wikipedia](https://en.wikipedia.org/wiki/France_in_the_Middle_Ages)\\nAt the end of the Middle Ages, France was the most populous region [clarification needed] in Europehaving overtaken Spain and Italy by 1340. [2] In the 14th century, before the arrival of the Black Death, the total population of the area covered by modern-day France has been estimated at 16 million. [3] The population of Paris is controversial. [4] ...\\n\\n[What was the capital of France before Paris? - StudyCountry.com](https://www.studycountry.com/wiki/what-was-the-capital-of-france-before-paris)\\nWhat was the original capital of France? Christianity arrived in the 2nd century AD, and the 5th century saw the end of Roman rule with the arrival of the Franks. In 508 AD, Clovis I, the Frankish king, united Gaul as a kingdom and established Paris as the capital, naming it in honor of the original Parisii tribe.\\n\\n[Kingdom of France - Wikipedia](https://en.wikipedia.org/wiki/Kingdom_of_France)\\nThe Kingdom of France is the historiographical name or umbrella term given to various political entities of France in the medieval and early modern period. It was one of the most powerful states in Europe from the High Middle Ages to 1848 during its dissolution. It was also an early colonial power, with colonies in Asia and Africa, and the largest being New France in North America centred ...\\n\\n[What happened to France\\'s other capitals? - The French History Podcast](https://www.thefrenchhistorypodcast.com/what-happened-to-frances-other-capitals/)\\nHowever, in France\\'s long history other cities have taken that role. In the modern period it\\'s very clear which cities are the capital as they hold the permanent seat of government. Before that, the capital was usually wherever the king most commonly held court. When Hugh Capet became king in 987 he ruled France from his powerbase in the north.\\n\\n[How did France get its name? - Geographic Pedia](https://www.ncesc.com/geographic-pedia/how-did-france-get-its-name/)\\nFrance Before its Name. ... Gaul was also referred to as Frankia when it was first conquered by the Franks in the 5th century AD. The Franks, a Germanic tribe, bestowed their name upon the region, eventually leading to the name \"France.\" ... In 987 A.D., Paris officially became the capital of France, solidifying its place in history. French ...\\n\\n[When did paris first become the capital of france?](https://www.reddit.com/r/AskHistorians/comments/18yz331/when_did_paris_first_become_the_capital_of_france/)\\nWhen did paris first become the capital of france? Okay having some articals online saying it was in 987 but i thought it was the capital during Charles the balds reign am I misremembering or is the internet oversimplifying things? ... Please read the rules before participating, as we remove all comments which break the rules. ...\\n\\n[Kingdom of West Francia Timeline - World History Encyclopedia](https://www.worldhistory.org/timeline/Kingdom_of_West_Francia/)\\nThe Kingdom of West Francia (843-987 CE, also known as The Kingdom of the West Franks) was the region of Western Europe that formed the western part of the Carolingian Empire of Charlemagne (Holy Roman Emperor 800-814 CE) known as Francia or the Kingdom of the Franks. ... Birth of Charles the Simple\\'s and Eadgifu\\'s son Louis (later Louis IV of ...\\n\\n[The birth of the french capital - History of Paris city](http://www.paris-city.fr/GB/paris-city/au-fil-du-temps/histoire.php)\\nBIRTH OF A CAPITAL Discover the history of Paris through the centuries Paris was founded in the 3rd century B.C. on le de la Cit by a community of Celts. They were a group of tribal fishermen called the Parisii who, pushed by emigration towards the banks of the Seine, made a permanent settlement there and profited from the area\\'s fertility and temperate climate.\\nLast output from code snippet:\\nNone'}], tool_calls=[ToolCall(name='python_interpreter', arguments='final_answer(\"Laon\")', id='call_5')], start_time=1737104853.6312058, end_time=1737104856.750259, step=3, error=None, duration=3.1190531253814697, llm_output='Thought: Based on the search results, it is clear that the capital of France before Paris was Laon from 936 to 987 AD. However, before Laon, the capital was Soissons from 486 to 936 AD. Since the question asks for the capital before Paris, and Laon was the capital just before Paris, the answer should be Laon.\\n\\nCode:\\n```py\\nfinal_answer(\"Laon\")\\n```<end_code>', observations='Execution logs:\\nLast output from code snippet:\\nLaon', action_output='Laon')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the logs for the agent\n",
    "agent.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Knowledge Base Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tool that can query a vector database of technical documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
    "\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
    "    for doc in knowledge_base\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs_processed = text_splitter.split_documents(source_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Uses semantic search to retrieve parts of transformers documentation \" \\\n",
    "            \" that could be most relevant to the Q.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to the target documents. \" \\\n",
    "                \"Use the affirmative form rather than a question.\"\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, docs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.retriever = BM25Retriever.from_documents(\n",
    "            docs, k=10\n",
    "        )\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        docs = self.retriever.invoke(\n",
    "            query\n",
    "        )\n",
    "\n",
    "        return \"\\Retrieved documents: \\n\" + \"\".join(\n",
    "            [\n",
    "                f\"\\n\\n====== Document {str(i)} ======\\n\" + doc.page_content\n",
    "                for i, doc in enumerate(docs)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = RetrieverTool(docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\Retrieved documents: \\n\\n\\n====== Document 0 ======\\n### How do I make my model XLA compatible?\\n\\nIn many cases, your code is probably XLA-compatible already! However, there are a few things that work in normal TensorFlow that dont work in XLA. Weve distilled them into three core rules below:\\n\\n<Tip>\\n\\n====== Document 1 ======\\n</Tip>\\n\\n<Tip warning={true}>\\n\\n**Tip born of painful experience:** Although using `jit_compile=True` is a good way to get a speed boost and test if your CPU/GPU code is XLA-compatible, it can actually cause a lot of problems if you leave it in when actually training on TPU. XLA compilation will happen implicitly on TPU, so remember to remove that line before actually running your code on a TPU!\\n\\n</Tip>\\n\\n### How do I make my model XLA compatible?\\n\\n====== Document 2 ======\\nIn particular all \"Please explain\" questions or objectively very user-specific feature requests belong to the forums. Here are some example of such questions:\\n\\n* \"I would like to use a BertModel within a RL-Agent for a customer support service. How can I use a BertForMaskedLM in my ChatBotModel?\"\\n\\n* \"Could you please explain why T5 has no positional embedding matrix under T5Model?\"\\n\\n* \"How should I set my generation parameters for translation?\"\\n\\n* \"How to train T5 on De->En translation?\"\\n\\n====== Document 3 ======\\nThe \"user\", \"system\" and \"assistant\" roles are the standard for chat, and we recommend using them when it makes sense,\\nparticularly if you want your model to operate well with [`ConversationalPipeline`]. However, you are not limited\\nto these roles - templating is extremely flexible, and any string can be a role.\\n\\n### I want to add some chat templates! How should I get started?\\n\\n====== Document 4 ======\\nNote that this time, the tokenizer has added the control tokens [INST] and [/INST] to indicate the start and end of \\nuser messages (but not assistant messages!). Mistral-instruct was trained with these tokens, but BlenderBot was not.\\n\\n## How do I use chat templates?\\n\\n====== Document 5 ======\\n>>> chat = [\\n...    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\\n...    {\"role\": \"assistant\", \"content\": \"I\\'m doing great. How can I help you today?\"},\\n...    {\"role\": \"user\", \"content\": \"I\\'d like to show off how chat templating works!\"},\\n... ]\\n\\n>>> tokenizer.apply_chat_template(chat, tokenize=False)\\n\" Hello, how are you?  I\\'m doing great. How can I help you today?   I\\'d like to show off how chat templating works!</s>\"\\n```\\n\\n====== Document 6 ======\\n>>> chat = [\\n...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\\n...   {\"role\": \"assistant\", \"content\": \"I\\'m doing great. How can I help you today?\"},\\n...   {\"role\": \"user\", \"content\": \"I\\'d like to show off how chat templating works!\"},\\n... ]\\n\\n>>> tokenizer.apply_chat_template(chat, tokenize=False)\\n\"<s>[INST] Hello, how are you? [/INST]I\\'m doing great. How can I help you today?</s> [INST] I\\'d like to show off how chat templating works! [/INST]\"\\n```\\n\\n====== Document 7 ======\\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)\\n>>> tokenizer.batch_decode(generated_ids)[0]\\n\\'</s>A chat between a curious human and the Statue of Liberty.\\\\n\\\\nHuman: What is your name?\\\\nStatue: I am the Statue of Liberty.\\\\nHuman: Where do you live?\\\\nStatue: New York City.\\\\nHuman: How long have you lived there?\\\\nStatue: I have lived here for about a year.\\\\nHuman: What is your favorite place to eat?\\\\nStatue: I love\\'\\n```\\n\\n### Expected speedups\\n\\n====== Document 8 ======\\nWhile the exact amount of speed-up is very much model-dependent, for TensorFlow text generation models inside  Transformers, we noticed a speed-up of ~100x. This document will explain how you can use XLA for these models to get the maximum amount of performance. Well also provide links to additional resources if youre interested to learn more about the benchmarks and our design philosophy behind the XLA integration.\\n\\n## Running TF functions with XLA\\n\\n====== Document 9 ======\\nThis code has exactly the same effect as the code above, but by avoiding a conditional, we ensure it will compile with XLA without problems!\\n\\n#### XLA Rule #2: Your code cannot have data-dependent shapes'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool(\"How should I use XLA with Huggingface?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\Retrieved documents: \\n\\n\\n====== Document 0 ======\\nand the [Kleister-NDA](https://github.com/applicaai/kleister-nda) dataset (a collection of non-disclosure\\n  agreements from the EDGAR database, including 254 documents for training, 83 documents for validation, and 203\\n  documents for testing).\\n- document image classification: the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset (a collection of\\n  400,000 images belonging to one of 16 classes).\\n\\n====== Document 1 ======\\nYou can enable gradient accumulation by adding the `gradient_accumulation_steps` argument to  [`TrainingArguments`]: \\n\\n```py\\ntraining_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\\n```\\n\\nIn the above example, your effective batch size becomes 4. \\n\\nAlternatively, use  Accelerate to gain full control over the training loop. Find the  Accelerate example \\n[further down in this guide](#using-accelerate).\\n\\n====== Document 2 ======\\n1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n\\n====== Document 3 ======\\n1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n\\n====== Document 4 ======\\n1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n\\n====== Document 5 ======\\n1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n\\n====== Document 6 ======\\n1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n\\n====== Document 7 ======\\n1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n\\n====== Document 8 ======\\n1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n\\n====== Document 9 ======\\n1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool(\"Find all the documents by Mostofa Patwary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
